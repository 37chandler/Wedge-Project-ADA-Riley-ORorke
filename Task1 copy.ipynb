{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Building a Transaction Database in Google BigQuery  \n",
    "  \n",
    "#### Overview  \n",
    "The aim of this task is to programmatically upload Wedge transaction records to Google BigQuery. This process involves extracting transaction data from zipped files, ensuring correct column data types, and handling null values before uploading the data to BigQuery.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Extract the Main Zip File  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping transArchive_201001_201003.zip, already exists.\n",
      "Skipping transArchive_201004_201006.zip, already exists.\n",
      "Skipping transArchive_201007_201009.zip, already exists.\n",
      "Skipping transArchive_201010_201012.zip, already exists.\n",
      "Skipping transArchive_201101_201103.zip, already exists.\n",
      "Skipping transArchive_201104.zip, already exists.\n",
      "Skipping transArchive_201105.zip, already exists.\n",
      "Skipping transArchive_201106.zip, already exists.\n",
      "Skipping transArchive_201107_201109.zip, already exists.\n",
      "Skipping transArchive_201110_201112.zip, already exists.\n",
      "Skipping transArchive_201201_201203.zip, already exists.\n",
      "Extracted transArchive_201201_201203_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201201_201203_inactive.zip\n",
      "Skipping transArchive_201204_201206.zip, already exists.\n",
      "Extracted transArchive_201204_201206_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201204_201206_inactive.zip\n",
      "Skipping transArchive_201207_201209.zip, already exists.\n",
      "Extracted transArchive_201207_201209_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201207_201209_inactive.zip\n",
      "Skipping transArchive_201210_201212.zip, already exists.\n",
      "Extracted transArchive_201210_201212_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201210_201212_inactive.zip\n",
      "Skipping transArchive_201301_201303.zip, already exists.\n",
      "Extracted transArchive_201301_201303_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201301_201303_inactive.zip\n",
      "Skipping transArchive_201304_201306.zip, already exists.\n",
      "Extracted transArchive_201304_201306_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201304_201306_inactive.zip\n",
      "Skipping transArchive_201307_201309.zip, already exists.\n",
      "Extracted transArchive_201307_201309_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201307_201309_inactive.zip\n",
      "Skipping transArchive_201310_201312.zip, already exists.\n",
      "Extracted transArchive_201310_201312_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201310_201312_inactive.zip\n",
      "Skipping transArchive_201401_201403.zip, already exists.\n",
      "Extracted transArchive_201401_201403_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201401_201403_inactive.zip\n",
      "Skipping transArchive_201404_201406.zip, already exists.\n",
      "Extracted transArchive_201404_201406_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201404_201406_inactive.zip\n",
      "Skipping transArchive_201407_201409.zip, already exists.\n",
      "Extracted transArchive_201407_201409_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201407_201409_inactive.zip\n",
      "Skipping transArchive_201410_201412.zip, already exists.\n",
      "Extracted transArchive_201410_201412_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201410_201412_inactive.zip\n",
      "Skipping transArchive_201501_201503.zip, already exists.\n",
      "Skipping transArchive_201504_201506.zip, already exists.\n",
      "Skipping transArchive_201507_201509.zip, already exists.\n",
      "Skipping transArchive_201510.zip, already exists.\n",
      "Skipping transArchive_201511.zip, already exists.\n",
      "Skipping transArchive_201512.zip, already exists.\n",
      "Skipping transArchive_201601.zip, already exists.\n",
      "Skipping transArchive_201602.zip, already exists.\n",
      "Skipping transArchive_201603.zip, already exists.\n",
      "Skipping transArchive_201604.zip, already exists.\n",
      "Skipping transArchive_201605.zip, already exists.\n",
      "Skipping transArchive_201606.zip, already exists.\n",
      "Skipping transArchive_201607.zip, already exists.\n",
      "Skipping transArchive_201608.zip, already exists.\n",
      "Skipping transArchive_201609.zip, already exists.\n",
      "Skipping transArchive_201610.zip, already exists.\n",
      "Skipping transArchive_201611.zip, already exists.\n",
      "Skipping transArchive_201612.zip, already exists.\n",
      "Skipping transArchive_201701.zip, already exists.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def extract_main_zip(main_zip_file, extract_to_folder):\n",
    "        \"\"\"\n",
    "    Extracts the contents of the main zip file to the specified folder.\n",
    "    \n",
    "    Parameters:\n",
    "    - main_zip_file (str): Path to the main zip file.\n",
    "    - extract_to_folder (str): Folder where the extracted contents will be saved.\n",
    "\n",
    "    The function handles the following:\n",
    "    - Skips files that already exist.\n",
    "    - Creates directories as needed.\n",
    "    - Extracts each file from the zip archive.\n",
    "    \"\"\"\n",
    "    # Ensure the extraction folder exists\n",
    "    os.makedirs(extract_to_folder, exist_ok=True)\n",
    "\n",
    "    # Open the main zip file\n",
    "    with zipfile.ZipFile(main_zip_file, 'r') as main_zip:\n",
    "        # Loop through all files in the main zip file\n",
    "        for zip_info in main_zip.infolist():\n",
    "            # Create the full output path\n",
    "            output_file_path = os.path.join(extract_to_folder, zip_info.filename)\n",
    "            \n",
    "            # Check if the file or folder already exists\n",
    "            if os.path.exists(output_file_path):\n",
    "                print(f\"Skipping {zip_info.filename}, already exists.\")\n",
    "                continue\n",
    "\n",
    "            # Check if it's a directory, then create it without extraction\n",
    "            if zip_info.is_dir():\n",
    "                os.makedirs(output_file_path, exist_ok=True)\n",
    "                print(f\"Created directory {output_file_path}\")\n",
    "            else:\n",
    "                # Create any necessary directories\n",
    "                os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "                \n",
    "                # Extract the file\n",
    "                with main_zip.open(zip_info) as source, open(output_file_path, 'wb') as target:\n",
    "                    shutil.copyfileobj(source, target)\n",
    "                print(f\"Extracted {zip_info.filename} to {output_file_path}\")\n",
    "\n",
    "# Input definitions: \n",
    "main_zip = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/WedgeZipOfZips.zip'\n",
    "extract_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip'\n",
    "\n",
    "extract_main_zip(main_zip, extract_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Extract the Nested Zip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping transArchive_201001_201003.csv, already exists.\n",
      "Skipping transArchive_201004_201006.csv, already exists.\n",
      "Skipping transArchive_201007_201009.csv, already exists.\n",
      "Skipping transArchive_201010_201012.csv, already exists.\n",
      "Skipping transArchive_201101_201103.csv, already exists.\n",
      "Skipping transArchive_201104.csv, already exists.\n",
      "Skipping transArchive_201105.csv, already exists.\n",
      "Skipping transArchive_201106.csv, already exists.\n",
      "Skipping transArchive_201107_201109.csv, already exists.\n",
      "Skipping transArchive_201110_201112.csv, already exists.\n",
      "Skipping transArchive_201201_201203.csv, already exists.\n",
      "Extracted transArchive_201201_201203_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201204_201206.csv, already exists.\n",
      "Extracted transArchive_201204_201206_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201207_201209.csv, already exists.\n",
      "Extracted transArchive_201207_201209_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201210_201212.csv, already exists.\n",
      "Extracted transArchive_201210_201212_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201301_201303.csv, already exists.\n",
      "Extracted transArchive_201301_201303_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201304_201306.csv, already exists.\n",
      "Extracted transArchive_201304_201306_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201307_201309.csv, already exists.\n",
      "Extracted transArchive_201307_201309_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201310_201312.csv, already exists.\n",
      "Extracted transArchive_201310_201312_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201401_201403.csv, already exists.\n",
      "Extracted transArchive_201401_201403_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201404_201406.csv, already exists.\n",
      "Extracted transArchive_201404_201406_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201407_201409.csv, already exists.\n",
      "Extracted transArchive_201407_201409_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201410_201412.csv, already exists.\n",
      "Extracted transArchive_201410_201412_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201501_201503.csv, already exists.\n",
      "Skipping transArchive_201504_201506.csv, already exists.\n",
      "Skipping transArchive_201507_201509.csv, already exists.\n",
      "Skipping transArchive_201510.csv, already exists.\n",
      "Skipping transArchive_201511.csv, already exists.\n",
      "Skipping transArchive_201512.csv, already exists.\n",
      "Skipping transArchive_201601.csv, already exists.\n",
      "Skipping transArchive_201602.csv, already exists.\n",
      "Skipping transArchive_201603.csv, already exists.\n",
      "Skipping transArchive_201604.csv, already exists.\n",
      "Skipping transArchive_201605.csv, already exists.\n",
      "Skipping transArchive_201606.csv, already exists.\n",
      "Skipping transArchive_201607.csv, already exists.\n",
      "Skipping transArchive_201608.csv, already exists.\n",
      "Skipping transArchive_201609.csv, already exists.\n",
      "Skipping transArchive_201610.csv, already exists.\n",
      "Skipping transArchive_201611.csv, already exists.\n",
      "Skipping transArchive_201612.csv, already exists.\n",
      "Skipping transArchive_201701.csv, already exists.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def extract_all_csvs_to_one_folder(extract_folder, output_folder):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Walk through the extracted folder and look for zip files\n",
    "    for root, dirs, files in os.walk(extract_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.zip'):\n",
    "                nested_zip_path = os.path.join(root, file)\n",
    "                \n",
    "                # Check if the file is a valid zip file before proceeding\n",
    "                try:\n",
    "                    with zipfile.ZipFile(nested_zip_path, 'r') as nested_zip:\n",
    "                        for zip_info in nested_zip.infolist():\n",
    "                            if zip_info.filename.endswith('.csv'):\n",
    "                                output_file_path = os.path.join(output_folder, zip_info.filename)\n",
    "                                # Check if the CSV file already exists in the output folder\n",
    "                                if not os.path.exists(output_file_path):\n",
    "                                    # Extract the CSV if it doesn't already exist\n",
    "                                    nested_zip.extract(zip_info, output_folder)\n",
    "                                    print(f\"Extracted {zip_info.filename} to {output_folder}\")\n",
    "                                else:\n",
    "                                    print(f\"Skipping {zip_info.filename}, already exists.\")\n",
    "                except zipfile.BadZipFile:\n",
    "                    print(f\"Skipping {nested_zip_path}, not a valid zip file.\")\n",
    "\n",
    "# Example usage\n",
    "extract_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip'\n",
    "output_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files'  \n",
    "\n",
    "extract_all_csvs_to_one_folder(extract_folder, output_folder)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Standardize and Clean the CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201001_201003.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201004_201006.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201007_201009.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201010_201012.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201101_201103.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201104.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201105.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201106.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201107_201109.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201110_201112.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201201_201203.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201201_201203_inactive.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201204_201206.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201204_201206_inactive.csv, already processed.\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201207_201209.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_9088\\1662963895.py:28: DtypeWarning: Columns (33,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, delimiter=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201207_201209_inactive.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201210_201212.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_9088\\1662963895.py:28: DtypeWarning: Columns (33,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, delimiter=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201210_201212_inactive.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201301_201303.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_9088\\1662963895.py:28: DtypeWarning: Columns (33,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, delimiter=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201301_201303_inactive.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201304_201306.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_9088\\1662963895.py:28: DtypeWarning: Columns (33,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, delimiter=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201304_201306_inactive.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201307_201309.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_9088\\1662963895.py:28: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, delimiter=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201307_201309_inactive.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201310_201312.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201310_201312_inactive.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201401_201403.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_9088\\1662963895.py:28: DtypeWarning: Columns (43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, delimiter=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201401_201403_inactive.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201404_201406.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201404_201406_inactive.csv\n",
      "Error: Could not parse D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201407_201409.csv.\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201407_201409_inactive.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201410_201412.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201410_201412_inactive.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201501_201503.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201504_201506.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201507_201509.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201510.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201511.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201512.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201601.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201602.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201603.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201604.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201605.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201606.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201607.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201608.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201609.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201610.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201611.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201612.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201701.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Path to the reference CSV for column names\n",
    "good_file_path = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/reference_files/transArchive_201001_201003_clean.csv'\n",
    "\n",
    "# Load the reference file to use its column names\n",
    "df_good = pd.read_csv(good_file_path)\n",
    "\n",
    "# Store the column names of the reference file for comparison\n",
    "good_column_names = df_good.columns.tolist()\n",
    "\n",
    "def clean_and_standardize_file(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Cleans and standardizes the input CSV file by handling delimiters, \n",
    "    NULL values, and column headers.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to load the file with automatic delimiter detection\n",
    "        try:\n",
    "            df = pd.read_csv(input_file, sep=None, engine='python')\n",
    "        except pd.errors.ParserError:\n",
    "            # Fallback to semicolon if automatic detection fails\n",
    "            df = pd.read_csv(input_file, delimiter=';')\n",
    "        \n",
    "        # If column count doesn't match, attempt to load with comma delimiter\n",
    "        if df.shape[1] != len(good_column_names):\n",
    "            df = pd.read_csv(input_file, delimiter=',')\n",
    "        \n",
    "        # Replace various representations of NULL with NaN/None\n",
    "        df.replace({\"NULL\": None, r\"\\\\N\": None, r\"\\N\": None}, inplace=True)\n",
    "        \n",
    "        # Check if the file's columns match the reference file's columns\n",
    "        if list(df.columns) != good_column_names:\n",
    "            # Check if the first row is a header and reorder columns accordingly\n",
    "            potential_header = df.iloc[0].tolist()\n",
    "            if set(potential_header) == set(good_column_names):\n",
    "                df.columns = potential_header\n",
    "                df = df.iloc[1:].reset_index(drop=True)  # Remove first row (now header)\n",
    "            else:\n",
    "                df.columns = good_column_names  # Set correct headers if missing\n",
    "\n",
    "        # Save the cleaned file with a comma delimiter\n",
    "        df.to_csv(output_file, index=False, sep=\",\")\n",
    "        print(f\"File cleaned and saved: {output_file}\")\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: {input_file} is empty.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error: Could not parse {input_file}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {input_file} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_file}: {e}\")\n",
    "\n",
    "def process_extracted_csvs(extracted_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Processes all CSV files in the extracted folder, cleaning and saving them\n",
    "    to the output folder. Skips files that have already been processed.\n",
    "\n",
    "    \"\"\"\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get all CSV files from the extracted folder (recursive search)\n",
    "    csv_files = glob.glob(f\"{extracted_folder}/**/*.csv\", recursive=True)\n",
    "    \n",
    "    # Process each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        output_file = os.path.join(output_folder, os.path.basename(csv_file))\n",
    "        \n",
    "        # Skip files that are already processed\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Skipping {csv_file}, already processed.\")\n",
    "            continue\n",
    "        \n",
    "        # Clean and standardize the file\n",
    "        clean_and_standardize_file(csv_file, output_file)\n",
    "\n",
    "# Example usage\n",
    "extracted_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files'  # Folder where the extracted CSVs are located\n",
    "output_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files'  # Folder to save cleaned CSVs\n",
    "\n",
    "# Process and clean the extracted CSVs\n",
    "process_extracted_csvs(extracted_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Uploading to Google BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52 CSV files to upload.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201001_201003.csv, table transArchive_201001_201003 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201004_201006.csv, table transArchive_201004_201006 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201007_201009.csv, table transArchive_201007_201009 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201010_201012.csv, table transArchive_201010_201012 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201101_201103.csv, table transArchive_201101_201103 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201104.csv, table transArchive_201104 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201105.csv, table transArchive_201105 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201106.csv, table transArchive_201106 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201107_201109.csv, table transArchive_201107_201109 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201110_201112.csv, table transArchive_201110_201112 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201201_201203.csv, table transArchive_201201_201203 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201201_201203_inactive.csv, table transArchive_201201_201203_inactive already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201204_201206.csv, table transArchive_201204_201206 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201204_201206_inactive.csv, table transArchive_201204_201206_inactive already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201207_201209.csv, table transArchive_201207_201209 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201207_201209_inactive.csv, table transArchive_201207_201209_inactive already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201210_201212.csv, table transArchive_201210_201212 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201210_201212_inactive.csv, table transArchive_201210_201212_inactive already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201301_201303.csv, table transArchive_201301_201303 already exists in BigQuery.\n",
      "Loaded 148623 rows into wedgeproject-rileyororke.transaction_tables.transArchive_201301_201303_inactive\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201304_201306.csv, table transArchive_201304_201306 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201304_201306_inactive.csv, table transArchive_201304_201306_inactive already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201307_201309.csv, table transArchive_201307_201309 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201307_201309_inactive.csv, table transArchive_201307_201309_inactive already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201310_201312.csv, table transArchive_201310_201312 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201310_201312_inactive.csv, table transArchive_201310_201312_inactive already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201401_201403.csv, table transArchive_201401_201403 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201401_201403_inactive.csv, table transArchive_201401_201403_inactive already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201404_201406.csv, table transArchive_201404_201406 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201404_201406_inactive.csv, table transArchive_201404_201406_inactive already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201407_201409_inactive.csv, table transArchive_201407_201409_inactive already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201410_201412.csv, table transArchive_201410_201412 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201410_201412_inactive.csv, table transArchive_201410_201412_inactive already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201501_201503.csv, table transArchive_201501_201503 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201504_201506.csv, table transArchive_201504_201506 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201507_201509.csv, table transArchive_201507_201509 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201510.csv, table transArchive_201510 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201511.csv, table transArchive_201511 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201512.csv, table transArchive_201512 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201601.csv, table transArchive_201601 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201602.csv, table transArchive_201602 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201603.csv, table transArchive_201603 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201604.csv, table transArchive_201604 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201605.csv, table transArchive_201605 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201606.csv, table transArchive_201606 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201607.csv, table transArchive_201607 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201608.csv, table transArchive_201608 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201609.csv, table transArchive_201609 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201610.csv, table transArchive_201610 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201611.csv, table transArchive_201611 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201612.csv, table transArchive_201612 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201701.csv, table transArchive_201701 already exists in BigQuery.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from google.cloud import bigquery\n",
    "\n",
    "def table_exists(client, dataset_id, table_name):\n",
    "    \"\"\"Check if a table already exists in BigQuery.\"\"\"\n",
    "    try:\n",
    "        client.get_table(f'{dataset_id}.{table_name}')\n",
    "        return True\n",
    "    except Exception:\n",
    "        # Table does not exist\n",
    "        return False\n",
    "\n",
    "def upload_csv_to_bigquery(client, dataset_id, table_name, csv_file):\n",
    "    try:\n",
    "        # Configure the load job with schema autodetection\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format=bigquery.SourceFormat.CSV,\n",
    "            skip_leading_rows=1,  # Skipping header row if the CSV contains headers\n",
    "            autodetect=True  # Automatically detect schema\n",
    "        )\n",
    "\n",
    "        # Load data from CSV into BigQuery\n",
    "        with open(csv_file, \"rb\") as source_file:\n",
    "            load_job = client.load_table_from_file(source_file, f'{dataset_id}.{table_name}', job_config=job_config)\n",
    "\n",
    "        # Wait for the load job to complete\n",
    "        load_job.result()\n",
    "\n",
    "        print(f\"Loaded {load_job.output_rows} rows into {dataset_id}.{table_name}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading {csv_file} to BigQuery: {e}\")\n",
    "\n",
    "def upload_all_csvs(input_folder, dataset_id):\n",
    "    # Initialize the BigQuery client\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Get all CSV files in the input folder\n",
    "    csv_files = glob.glob(os.path.join(input_folder, '*.csv'))\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to upload.\")\n",
    "    \n",
    "    # Iterate through each file and upload it\n",
    "    for csv_file in csv_files:\n",
    "        # Extract the base name of the CSV file to use as the table name\n",
    "        table_name = os.path.basename(csv_file).replace('.csv', '')\n",
    "        \n",
    "        # Check if the table already exists\n",
    "        if table_exists(client, dataset_id, table_name):\n",
    "            print(f\"Skipping {csv_file}, table {table_name} already exists in BigQuery.\")\n",
    "            continue\n",
    "        \n",
    "        # Upload the CSV to BigQuery if the table does not exist\n",
    "        upload_csv_to_bigquery(client, dataset_id, table_name, csv_file)\n",
    "\n",
    "# Example usage\n",
    "input_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files'  # Folder with cleaned CSVs\n",
    "dataset_id = 'wedgeproject-rileyororke.transaction_tables'  # Your BigQuery dataset\n",
    "\n",
    "# Upload all CSVs to BigQuery\n",
    "upload_all_csvs(input_folder, dataset_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values before cleaning: ['1' '0' nan '-1' '3' ' ' '1.0' '0.0' '-1.0' '3.0']\n",
      "Unique values after cleaning: [ 1.  0. nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_9088\\1567637848.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['organic'] = df['organic'].replace({1.0: 1, 0.0: 0, -1.0: None, 3.0: None})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file saved to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files/transArchive_201301_201303_inactive.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the problematic CSV with low_memory=False to handle mixed types better\n",
    "csv_file = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files/transArchive_201301_201303_inactive.csv'\n",
    "df = pd.read_csv(csv_file, low_memory=False)\n",
    "\n",
    "# Check for unexpected values in the 'organic' column\n",
    "print(f\"Unique values before cleaning: {df['organic'].unique()}\")\n",
    "\n",
    "# Convert 'organic' to numeric, coercing errors (which will convert invalid entries to NaN)\n",
    "df['organic'] = pd.to_numeric(df['organic'], errors='coerce')\n",
    "\n",
    "# Replace valid floating point numbers with integers (convert 1.0 to 1, 0.0 to 0)\n",
    "df['organic'] = df['organic'].replace({1.0: 1, 0.0: 0, -1.0: None, 3.0: None})\n",
    "\n",
    "# Check for unique values after cleaning\n",
    "print(f\"Unique values after cleaning: {df['organic'].unique()}\")\n",
    "\n",
    "# Save the cleaned file\n",
    "output_file = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files/transArchive_201301_201303_inactive.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Cleaned file saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
