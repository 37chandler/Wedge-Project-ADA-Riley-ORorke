{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Building a Transaction Database in Google BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview  \n",
    "In Task 1, the focus is on loading the raw Wedge Co-Op transaction data into Google BigQuery while addressing various data quality issues. This task involves managing inconsistencies such as mixed delimiters (commas and semicolons), different representations of null values (\"NULL\", \"\\N\", and \"\\\\N\"), and mismatched column headers. Python scripts are used to automate the Extract, Transform, and Load (ETL) process. The raw transaction files are first extracted from local directories. In the transformation stage, the script standardizes null values, corrects column headers, and ensures data type consistency across files, including proper handling of numeric fields and date-time formats. Once the data is cleaned and transformed, it is loaded into Google BigQuery with schema enforcement to ensure that the transactional data adheres to the expected structure. This process creates a clean, structured dataset in BigQuery, ready for analysis in the subsequent tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Extract  \n",
    "\n",
    "Extract Zipped Raw Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping transArchive_201001_201003.zip, already exists.\n",
      "Skipping transArchive_201004_201006.zip, already exists.\n",
      "Skipping transArchive_201007_201009.zip, already exists.\n",
      "Skipping transArchive_201010_201012.zip, already exists.\n",
      "Skipping transArchive_201101_201103.zip, already exists.\n",
      "Skipping transArchive_201104.zip, already exists.\n",
      "Skipping transArchive_201105.zip, already exists.\n",
      "Skipping transArchive_201106.zip, already exists.\n",
      "Skipping transArchive_201107_201109.zip, already exists.\n",
      "Skipping transArchive_201110_201112.zip, already exists.\n",
      "Skipping transArchive_201201_201203.zip, already exists.\n",
      "Skipping transArchive_201201_201203_inactive.zip, already exists.\n",
      "Skipping transArchive_201204_201206.zip, already exists.\n",
      "Skipping transArchive_201204_201206_inactive.zip, already exists.\n",
      "Skipping transArchive_201207_201209.zip, already exists.\n",
      "Skipping transArchive_201207_201209_inactive.zip, already exists.\n",
      "Skipping transArchive_201210_201212.zip, already exists.\n",
      "Skipping transArchive_201210_201212_inactive.zip, already exists.\n",
      "Skipping transArchive_201301_201303.zip, already exists.\n",
      "Skipping transArchive_201301_201303_inactive.zip, already exists.\n",
      "Skipping transArchive_201304_201306.zip, already exists.\n",
      "Skipping transArchive_201304_201306_inactive.zip, already exists.\n",
      "Skipping transArchive_201307_201309.zip, already exists.\n",
      "Skipping transArchive_201307_201309_inactive.zip, already exists.\n",
      "Skipping transArchive_201310_201312.zip, already exists.\n",
      "Skipping transArchive_201310_201312_inactive.zip, already exists.\n",
      "Skipping transArchive_201401_201403.zip, already exists.\n",
      "Skipping transArchive_201401_201403_inactive.zip, already exists.\n",
      "Skipping transArchive_201404_201406.zip, already exists.\n",
      "Skipping transArchive_201404_201406_inactive.zip, already exists.\n",
      "Skipping transArchive_201407_201409.zip, already exists.\n",
      "Skipping transArchive_201407_201409_inactive.zip, already exists.\n",
      "Skipping transArchive_201410_201412.zip, already exists.\n",
      "Skipping transArchive_201410_201412_inactive.zip, already exists.\n",
      "Skipping transArchive_201501_201503.zip, already exists.\n",
      "Skipping transArchive_201504_201506.zip, already exists.\n",
      "Skipping transArchive_201507_201509.zip, already exists.\n",
      "Skipping transArchive_201510.zip, already exists.\n",
      "Skipping transArchive_201511.zip, already exists.\n",
      "Skipping transArchive_201512.zip, already exists.\n",
      "Skipping transArchive_201601.zip, already exists.\n",
      "Skipping transArchive_201602.zip, already exists.\n",
      "Skipping transArchive_201603.zip, already exists.\n",
      "Skipping transArchive_201604.zip, already exists.\n",
      "Skipping transArchive_201605.zip, already exists.\n",
      "Skipping transArchive_201606.zip, already exists.\n",
      "Skipping transArchive_201607.zip, already exists.\n",
      "Skipping transArchive_201608.zip, already exists.\n",
      "Skipping transArchive_201609.zip, already exists.\n",
      "Skipping transArchive_201610.zip, already exists.\n",
      "Skipping transArchive_201611.zip, already exists.\n",
      "Skipping transArchive_201612.zip, already exists.\n",
      "Skipping transArchive_201701.zip, already exists.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def extract_main_zip(main_zip_file, extract_to_folder):\n",
    "    \"\"\"\n",
    "    Extracts the contents of the main zip file to the specified folder.\n",
    "    \n",
    "    Parameters:\n",
    "    - main_zip_file (str): Path to the main zip file.\n",
    "    - extract_to_folder (str): Folder where the extracted contents will be saved.\n",
    "\n",
    "    The function handles:\n",
    "    - Skipping files that already exist.\n",
    "    - Creating directories as needed.\n",
    "    - Extracting each file from the zip archive.\n",
    "    \"\"\"\n",
    "    # Ensure the extraction folder exists\n",
    "    os.makedirs(extract_to_folder, exist_ok=True)\n",
    "\n",
    "    # Open the main zip file\n",
    "    with zipfile.ZipFile(main_zip_file, 'r') as main_zip:\n",
    "        # Loop through all files in the main zip file\n",
    "        for zip_info in main_zip.infolist():\n",
    "            # Create the full output path\n",
    "            output_file_path = os.path.join(extract_to_folder, zip_info.filename)\n",
    "            \n",
    "            # Skip if the file or folder already exists\n",
    "            if os.path.exists(output_file_path):\n",
    "                print(f\"Skipping {zip_info.filename}, already exists.\")\n",
    "                continue\n",
    "\n",
    "            # Create directory if it's a folder in the archive\n",
    "            if zip_info.is_dir():\n",
    "                os.makedirs(output_file_path, exist_ok=True)\n",
    "                print(f\"Created directory {output_file_path}\")\n",
    "            else:\n",
    "                # Create necessary directories for files\n",
    "                os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "                \n",
    "                # Extract the file to the target location\n",
    "                with main_zip.open(zip_info) as source, open(output_file_path, 'wb') as target:\n",
    "                    shutil.copyfileobj(source, target)\n",
    "                print(f\"Extracted {zip_info.filename} to {output_file_path}\")\n",
    "\n",
    "# Input definitions\n",
    "main_zip = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/WedgeZipOfZips.zip'\n",
    "extract_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip'\n",
    "\n",
    "# Extract the main zip file\n",
    "extract_main_zip(main_zip, extract_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the Nested Zip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping transArchive_201001_201003.csv, already exists.\n",
      "Skipping transArchive_201004_201006.csv, already exists.\n",
      "Skipping transArchive_201007_201009.csv, already exists.\n",
      "Skipping transArchive_201010_201012.csv, already exists.\n",
      "Skipping transArchive_201101_201103.csv, already exists.\n",
      "Skipping transArchive_201104.csv, already exists.\n",
      "Skipping transArchive_201105.csv, already exists.\n",
      "Skipping transArchive_201106.csv, already exists.\n",
      "Skipping transArchive_201107_201109.csv, already exists.\n",
      "Skipping transArchive_201110_201112.csv, already exists.\n",
      "Skipping transArchive_201201_201203.csv, already exists.\n",
      "Skipping transArchive_201201_201203_inactive.csv, already exists.\n",
      "Skipping transArchive_201204_201206.csv, already exists.\n",
      "Skipping transArchive_201204_201206_inactive.csv, already exists.\n",
      "Skipping transArchive_201207_201209.csv, already exists.\n",
      "Skipping transArchive_201207_201209_inactive.csv, already exists.\n",
      "Skipping transArchive_201210_201212.csv, already exists.\n",
      "Skipping transArchive_201210_201212_inactive.csv, already exists.\n",
      "Skipping transArchive_201301_201303.csv, already exists.\n",
      "Skipping transArchive_201301_201303_inactive.csv, already exists.\n",
      "Skipping transArchive_201304_201306.csv, already exists.\n",
      "Skipping transArchive_201304_201306_inactive.csv, already exists.\n",
      "Skipping transArchive_201307_201309.csv, already exists.\n",
      "Skipping transArchive_201307_201309_inactive.csv, already exists.\n",
      "Skipping transArchive_201310_201312.csv, already exists.\n",
      "Skipping transArchive_201310_201312_inactive.csv, already exists.\n",
      "Skipping transArchive_201401_201403.csv, already exists.\n",
      "Skipping transArchive_201401_201403_inactive.csv, already exists.\n",
      "Skipping transArchive_201404_201406.csv, already exists.\n",
      "Skipping transArchive_201404_201406_inactive.csv, already exists.\n",
      "Skipping transArchive_201407_201409.csv, already exists.\n",
      "Skipping transArchive_201407_201409_inactive.csv, already exists.\n",
      "Skipping transArchive_201410_201412.csv, already exists.\n",
      "Skipping transArchive_201410_201412_inactive.csv, already exists.\n",
      "Skipping transArchive_201501_201503.csv, already exists.\n",
      "Skipping transArchive_201504_201506.csv, already exists.\n",
      "Skipping transArchive_201507_201509.csv, already exists.\n",
      "Skipping transArchive_201510.csv, already exists.\n",
      "Skipping transArchive_201511.csv, already exists.\n",
      "Skipping transArchive_201512.csv, already exists.\n",
      "Skipping transArchive_201601.csv, already exists.\n",
      "Skipping transArchive_201602.csv, already exists.\n",
      "Skipping transArchive_201603.csv, already exists.\n",
      "Skipping transArchive_201604.csv, already exists.\n",
      "Skipping transArchive_201605.csv, already exists.\n",
      "Skipping transArchive_201606.csv, already exists.\n",
      "Skipping transArchive_201607.csv, already exists.\n",
      "Skipping transArchive_201608.csv, already exists.\n",
      "Skipping transArchive_201609.csv, already exists.\n",
      "Skipping transArchive_201610.csv, already exists.\n",
      "Skipping transArchive_201611.csv, already exists.\n",
      "Skipping transArchive_201612.csv, already exists.\n",
      "Skipping transArchive_201701.csv, already exists.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def extract_all_csvs_to_one_folder(extract_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Extracts all CSV files from nested zip files in the specified folder and saves them to a single output folder.\n",
    "\n",
    "    Parameters:\n",
    "    - extract_folder (str): Folder containing the nested zip files.\n",
    "    - output_folder (str): Folder where the extracted CSV files will be saved.\n",
    "\n",
    "    The function handles:\n",
    "    - Walking through directories to find and extract CSV files from nested zip files.\n",
    "    - Skipping CSV files that already exist in the output folder.\n",
    "    - Handling invalid zip files.\n",
    "    \"\"\"\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Walk through the extracted folder and look for zip files\n",
    "    for root, dirs, files in os.walk(extract_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.zip'):\n",
    "                nested_zip_path = os.path.join(root, file)\n",
    "                \n",
    "                # Check if the file is a valid zip file before proceeding\n",
    "                try:\n",
    "                    with zipfile.ZipFile(nested_zip_path, 'r') as nested_zip:\n",
    "                        # Extract CSV files from the nested zip\n",
    "                        for zip_info in nested_zip.infolist():\n",
    "                            if zip_info.filename.endswith('.csv'):\n",
    "                                output_file_path = os.path.join(output_folder, zip_info.filename)\n",
    "                                \n",
    "                                # Skip CSV files that already exist\n",
    "                                if not os.path.exists(output_file_path):\n",
    "                                    nested_zip.extract(zip_info, output_folder)\n",
    "                                    print(f\"Extracted {zip_info.filename} to {output_folder}\")\n",
    "                                else:\n",
    "                                    print(f\"Skipping {zip_info.filename}, already exists.\")\n",
    "                except zipfile.BadZipFile:\n",
    "                    print(f\"Skipping {nested_zip_path}, not a valid zip file.\")\n",
    "\n",
    "# Input definitions\n",
    "extract_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip'\n",
    "output_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files'  \n",
    "\n",
    "# Extract all CSVs from nested zip files\n",
    "extract_all_csvs_to_one_folder(extract_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Transform  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53 CSV files to process.\n",
      "Skipping already processed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201001_201003.csv\n",
      "Skipping already processed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201004_201006.csv\n",
      "Skipping already processed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201007_201009.csv\n",
      "Skipping already processed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201010_201012.csv\n",
      "Skipping already processed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201101_201103.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 91\u001b[0m\n\u001b[0;32m     88\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Process the files\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m \u001b[43mprocess_all_csv_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[75], line 78\u001b[0m, in \u001b[0;36mprocess_all_csv_files\u001b[1;34m(input_folder, output_folder)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# Clean and save the file\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     \u001b[43mclean_csv_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# List all saved files in the output folder\u001b[39;00m\n\u001b[0;32m     81\u001b[0m saved_files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/*.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[75], line 20\u001b[0m, in \u001b[0;36mclean_csv_file\u001b[1;34m(input_file, output_file)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Attempt to load the file with automatic delimiter detection\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pd\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mParserError:\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;66;03m# If automatic detection fails, try with semicolon delimiter\u001b[39;00m\n\u001b[0;32m     23\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(input_file, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\python_parser.py:291\u001b[0m, in \u001b[0;36mPythonParser.read\u001b[1;34m(self, rows)\u001b[0m\n\u001b[0;32m    288\u001b[0m alldata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rows_to_cols(content)\n\u001b[0;32m    289\u001b[0m data, columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exclude_implicit_index(alldata)\n\u001b[1;32m--> 291\u001b[0m conv_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m columns, conv_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_date_conversions(columns, conv_data)\n\u001b[0;32m    294\u001b[0m index, result_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_index(\n\u001b[0;32m    295\u001b[0m     conv_data, alldata, columns, indexnamerow\n\u001b[0;32m    296\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\python_parser.py:362\u001b[0m, in \u001b[0;36mPythonParser._convert_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    359\u001b[0m     clean_na_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mna_values\n\u001b[0;32m    360\u001b[0m     clean_na_fvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mna_fvalues\n\u001b[1;32m--> 362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_to_ndarrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_na_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_na_fvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_conv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\base_parser.py:580\u001b[0m, in \u001b[0;36mParserBase._convert_to_ndarrays\u001b[1;34m(self, dct, na_values, na_fvalues, verbose, converters, dtypes)\u001b[0m\n\u001b[0;32m    577\u001b[0m try_num_bool \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m (cast_type \u001b[38;5;129;01mand\u001b[39;00m is_str_or_ea_dtype)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;66;03m# general type inference and conversion\u001b[39;00m\n\u001b[1;32m--> 580\u001b[0m cvals, na_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_types\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcol_na_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcol_na_fvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_num_bool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;66;03m# type specified in dtype param or cast_type is an EA\u001b[39;00m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cast_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\base_parser.py:740\u001b[0m, in \u001b[0;36mParserBase._infer_types\u001b[1;34m(self, values, na_values, no_dtype_specified, try_num_bool)\u001b[0m\n\u001b[0;32m    738\u001b[0m             na_count \u001b[38;5;241m=\u001b[39m result_mask\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    739\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 740\u001b[0m             na_count \u001b[38;5;241m=\u001b[39m \u001b[43misna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    742\u001b[0m     result \u001b[38;5;241m=\u001b[39m values\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\dtypes\\missing.py:101\u001b[0m, in \u001b[0;36misna\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@overload\u001b[39m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misna\u001b[39m(obj: \u001b[38;5;28mobject\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mbool_] \u001b[38;5;241m|\u001b[39m NDFrame:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misna\u001b[39m(obj: \u001b[38;5;28mobject\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mbool_] \u001b[38;5;241m|\u001b[39m NDFrame:\n\u001b[0;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    Detect missing values for an array-like object.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m    Name: 1, dtype: bool\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _isna(obj)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Path to the reference file for column headers\n",
    "reference_file_path = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/reference_files/transArchive_201001_201003_clean.csv'\n",
    "\n",
    "# Load the reference file to get the correct column headers\n",
    "df_reference = pd.read_csv(reference_file_path)\n",
    "reference_columns = df_reference.columns.tolist()\n",
    "\n",
    "def clean_csv_file(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Cleans and standardizes the input CSV file, ensuring the correct headers and replacing NULL values.\n",
    "    Saves the cleaned file to the specified output location.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to load the file with automatic delimiter detection\n",
    "        try:\n",
    "            df = pd.read_csv(input_file, sep=None, engine='python')\n",
    "        except pd.errors.ParserError:\n",
    "            # If automatic detection fails, try with semicolon delimiter\n",
    "            df = pd.read_csv(input_file, delimiter=';')\n",
    "        \n",
    "        # If column count doesn't match the reference, try loading with a comma delimiter\n",
    "        if df.shape[1] != len(reference_columns):\n",
    "            df = pd.read_csv(input_file, delimiter=',')\n",
    "        \n",
    "        # Replace various NULL representations with None/NaN\n",
    "        df.replace({\"NULL\": None, r\"\\\\N\": None, r\"\\N\": None}, inplace=True)\n",
    "        \n",
    "        # Ensure the correct headers\n",
    "        if list(df.columns) != reference_columns:\n",
    "            first_row_as_header = df.iloc[0].tolist()\n",
    "            if set(first_row_as_header) == set(reference_columns):\n",
    "                # If the first row matches the reference, use it as headers\n",
    "                df.columns = first_row_as_header\n",
    "                df = df.iloc[1:].reset_index(drop=True)\n",
    "            else:\n",
    "                # Otherwise, apply the reference headers\n",
    "                df.columns = reference_columns\n",
    "        \n",
    "        # Save the cleaned CSV file\n",
    "        df.to_csv(output_file, index=False, sep=\",\")\n",
    "        print(f\"File cleaned and saved: {output_file}\")\n",
    "    \n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: {input_file} is empty.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error: Could not parse {input_file}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {input_file} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_file}: {e}\")\n",
    "\n",
    "def process_all_csv_files(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Processes all CSV files in the input folder, cleaning and standardizing them before saving to the output folder.\n",
    "    Skips files that have already been processed and saved in the output folder.\n",
    "    \"\"\"\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get all CSV files from the input folder\n",
    "    csv_files = glob.glob(f\"{input_folder}/**/*.csv\", recursive=True)\n",
    "    print(f\"Found {len(csv_files)} CSV files to process.\")\n",
    "    \n",
    "    # Process each file\n",
    "    for csv_file in csv_files:\n",
    "        output_file = os.path.join(output_folder, os.path.basename(csv_file))\n",
    "        \n",
    "        # Skip files that have already been processed\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Skipping already processed file: {output_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Clean and save the file\n",
    "        clean_csv_file(csv_file, output_file)\n",
    "    \n",
    "    # List all saved files in the output folder\n",
    "    saved_files = glob.glob(f\"{output_folder}/*.csv\")\n",
    "    print(f\"\\nSaved {len(saved_files)} files to {output_folder}:\")\n",
    "    for file in saved_files:\n",
    "        print(file)\n",
    "\n",
    "# Input definitions\n",
    "input_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files'\n",
    "output_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files'\n",
    "\n",
    "# Process the files\n",
    "process_all_csv_files(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "\n",
    "# Initialize BigQuery client with the correct Project ID\n",
    "client = bigquery.Client(project='wedgeproject-rileyororke')\n",
    "\n",
    "# Define the dataset ID and create the dataset if it doesn't exist\n",
    "dataset_id = 'wedgeproject-rileyororke.transaction_tables'\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "client.create_dataset(dataset, exists_ok=True)\n",
    "\n",
    "# Path to the folder containing the CSV files\n",
    "folder_path = r\"D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/BigClean\"\n",
    "\n",
    "# Schema definition for the transaction table\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"datetime\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"register_no\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"emp_no\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"trans_no\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"upc\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"description\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"trans_type\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"trans_subtype\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"trans_status\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"department\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"quantity\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"Scale\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"cost\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"unitPrice\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"total\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"regPrice\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"altPrice\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"tax\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"taxexempt\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"foodstamp\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"wicable\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"discount\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"memDiscount\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"discountable\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"discounttype\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"voided\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"percentDiscount\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"ItemQtty\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"volDiscType\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"volume\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"VolSpecial\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"mixMatch\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"matched\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"memType\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"staff\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"numflag\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"itemstatus\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"tenderstatus\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"charflag\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"varflag\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"batchHeaderID\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"local\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"organic\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"display\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"receipt\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"card_no\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"store\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"branch\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"match_id\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"trans_id\", \"FLOAT\"),\n",
    "]\n",
    "\n",
    "def load_csv_to_bigquery(folder_path, dataset_id):\n",
    "    \"\"\"\n",
    "    Load all CSV files from the specified folder into BigQuery.\n",
    "    Each file is uploaded to a table named after the file (without extension).\n",
    "    Skips tables that already exist in BigQuery.\n",
    "    \"\"\"\n",
    "    # Iterate over all files in the folder and process CSV files\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Use the file name (without extension) as the table ID\n",
    "            table_id = os.path.splitext(file_name)[0]\n",
    "            \n",
    "            # Define the table reference within the dataset\n",
    "            table_ref = client.dataset('transaction_tables').table(table_id)\n",
    "\n",
    "            # Check if the table already exists\n",
    "            try:\n",
    "                client.get_table(table_ref)\n",
    "                print(f\"Skipping table {table_id}, it already exists.\")\n",
    "                continue  # Skip the rest of the loop if the table exists\n",
    "            except Exception:\n",
    "                # If the table does not exist, proceed with loading\n",
    "                pass\n",
    "\n",
    "            # Configure the load job for CSV format\n",
    "            job_config = bigquery.LoadJobConfig(\n",
    "                source_format=bigquery.SourceFormat.CSV,\n",
    "                schema=schema,\n",
    "                skip_leading_rows=1  # Skip header row\n",
    "            )\n",
    "\n",
    "            # Open the file and load it into BigQuery\n",
    "            with open(file_path, 'rb') as file:\n",
    "                load_job = client.load_table_from_file(file, table_ref, job_config=job_config)\n",
    "                load_job.result()  # Wait for the job to complete\n",
    "            \n",
    "            print(f\"Loaded {file_name} into {dataset_id}.{table_id}\")\n",
    "\n",
    "# Input definitions\n",
    "folder_path = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files'\n",
    "\n",
    "# Run the function to load CSVs from the specified folder to BigQuery\n",
    "load_csv_to_bigquery(folder_path, dataset_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
