{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Building a Transaction Database in Google BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview  \n",
    "In Task 1, the focus is on loading the raw Wedge Co-Op transaction data into Google BigQuery while addressing various data quality issues. This task involves managing inconsistencies such as mixed delimiters (commas and semicolons), different representations of null values (\"NULL\", \"\\N\", and \"\\\\N\"), and mismatched column headers. Python scripts are used to automate the Extract, Transform, and Load (ETL) process. The raw transaction files are first extracted from local directories. In the transformation stage, the script standardizes null values, corrects column headers, and ensures data type consistency across files, including proper handling of numeric fields and date-time formats. Once the data is cleaned and transformed, it is loaded into Google BigQuery with schema enforcement to ensure that the transactional data adheres to the expected structure. This process creates a clean, structured dataset in BigQuery, ready for analysis in the subsequent tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Extract  \n",
    "\n",
    "Extract Zipped Raw Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping transArchive_201001_201003.zip, already exists.\n",
      "Skipping transArchive_201004_201006.zip, already exists.\n",
      "Skipping transArchive_201007_201009.zip, already exists.\n",
      "Skipping transArchive_201010_201012.zip, already exists.\n",
      "Skipping transArchive_201101_201103.zip, already exists.\n",
      "Skipping transArchive_201104.zip, already exists.\n",
      "Skipping transArchive_201105.zip, already exists.\n",
      "Skipping transArchive_201106.zip, already exists.\n",
      "Skipping transArchive_201107_201109.zip, already exists.\n",
      "Skipping transArchive_201110_201112.zip, already exists.\n",
      "Skipping transArchive_201201_201203.zip, already exists.\n",
      "Skipping transArchive_201201_201203_inactive.zip, already exists.\n",
      "Skipping transArchive_201204_201206.zip, already exists.\n",
      "Skipping transArchive_201204_201206_inactive.zip, already exists.\n",
      "Skipping transArchive_201207_201209.zip, already exists.\n",
      "Skipping transArchive_201207_201209_inactive.zip, already exists.\n",
      "Skipping transArchive_201210_201212.zip, already exists.\n",
      "Skipping transArchive_201210_201212_inactive.zip, already exists.\n",
      "Skipping transArchive_201301_201303.zip, already exists.\n",
      "Skipping transArchive_201301_201303_inactive.zip, already exists.\n",
      "Skipping transArchive_201304_201306.zip, already exists.\n",
      "Skipping transArchive_201304_201306_inactive.zip, already exists.\n",
      "Skipping transArchive_201307_201309.zip, already exists.\n",
      "Skipping transArchive_201307_201309_inactive.zip, already exists.\n",
      "Skipping transArchive_201310_201312.zip, already exists.\n",
      "Skipping transArchive_201310_201312_inactive.zip, already exists.\n",
      "Skipping transArchive_201401_201403.zip, already exists.\n",
      "Skipping transArchive_201401_201403_inactive.zip, already exists.\n",
      "Skipping transArchive_201404_201406.zip, already exists.\n",
      "Skipping transArchive_201404_201406_inactive.zip, already exists.\n",
      "Skipping transArchive_201407_201409.zip, already exists.\n",
      "Skipping transArchive_201407_201409_inactive.zip, already exists.\n",
      "Skipping transArchive_201410_201412.zip, already exists.\n",
      "Skipping transArchive_201410_201412_inactive.zip, already exists.\n",
      "Skipping transArchive_201501_201503.zip, already exists.\n",
      "Skipping transArchive_201504_201506.zip, already exists.\n",
      "Skipping transArchive_201507_201509.zip, already exists.\n",
      "Skipping transArchive_201510.zip, already exists.\n",
      "Skipping transArchive_201511.zip, already exists.\n",
      "Skipping transArchive_201512.zip, already exists.\n",
      "Skipping transArchive_201601.zip, already exists.\n",
      "Skipping transArchive_201602.zip, already exists.\n",
      "Skipping transArchive_201603.zip, already exists.\n",
      "Skipping transArchive_201604.zip, already exists.\n",
      "Skipping transArchive_201605.zip, already exists.\n",
      "Skipping transArchive_201606.zip, already exists.\n",
      "Skipping transArchive_201607.zip, already exists.\n",
      "Skipping transArchive_201608.zip, already exists.\n",
      "Skipping transArchive_201609.zip, already exists.\n",
      "Skipping transArchive_201610.zip, already exists.\n",
      "Skipping transArchive_201611.zip, already exists.\n",
      "Skipping transArchive_201612.zip, already exists.\n",
      "Skipping transArchive_201701.zip, already exists.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def extract_zip_file(zip_file_path, destination_folder):\n",
    "    \"\"\"\n",
    "    Extracts the contents of a zip file to the specified destination folder.\n",
    "    \n",
    "    Parameters:\n",
    "    - zip_file_path (str): The path to the zip file to be extracted.\n",
    "    - destination_folder (str): The folder where the contents of the zip file will be extracted.\n",
    "\n",
    "    The function handles:\n",
    "    - Skipping extraction of files that already exist.\n",
    "    - Creating necessary directories if they don't exist.\n",
    "    - Extracting files and directories from the zip archive.\n",
    "    \"\"\"\n",
    "    # Ensure the destination folder exists\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "    # Open the zip file\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "        # Loop through each file in the zip archive\n",
    "        for zip_info in zip_file.infolist():\n",
    "            # Determine the output file path\n",
    "            extracted_file_path = os.path.join(destination_folder, zip_info.filename)\n",
    "            \n",
    "            # Skip extraction if the file or folder already exists\n",
    "            if os.path.exists(extracted_file_path):\n",
    "                print(f\"Skipping {zip_info.filename}, it already exists.\")\n",
    "                continue\n",
    "\n",
    "            # If the entry is a directory, create it\n",
    "            if zip_info.is_dir():\n",
    "                os.makedirs(extracted_file_path, exist_ok=True)\n",
    "                print(f\"Created directory {extracted_file_path}\")\n",
    "            else:\n",
    "                # Ensure the parent directories for the file exist\n",
    "                os.makedirs(os.path.dirname(extracted_file_path), exist_ok=True)\n",
    "                \n",
    "                # Extract the file\n",
    "                with zip_file.open(zip_info) as source_file, open(extracted_file_path, 'wb') as target_file:\n",
    "                    shutil.copyfileobj(source_file, target_file)\n",
    "                print(f\"Extracted {zip_info.filename} to {extracted_file_path}\")\n",
    "\n",
    "# Define the input paths\n",
    "main_zip_file_path = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/WedgeZipOfZips.zip'\n",
    "extracted_folder_path = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip'\n",
    "\n",
    "# Extract the main zip file\n",
    "extract_zip_file(main_zip_file_path, extracted_folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the Nested Zip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping transArchive_201001_201003.csv, already exists.\n",
      "Skipping transArchive_201004_201006.csv, already exists.\n",
      "Skipping transArchive_201007_201009.csv, already exists.\n",
      "Skipping transArchive_201010_201012.csv, already exists.\n",
      "Skipping transArchive_201101_201103.csv, already exists.\n",
      "Skipping transArchive_201104.csv, already exists.\n",
      "Skipping transArchive_201105.csv, already exists.\n",
      "Skipping transArchive_201106.csv, already exists.\n",
      "Skipping transArchive_201107_201109.csv, already exists.\n",
      "Skipping transArchive_201110_201112.csv, already exists.\n",
      "Skipping transArchive_201201_201203.csv, already exists.\n",
      "Skipping transArchive_201201_201203_inactive.csv, already exists.\n",
      "Skipping transArchive_201204_201206.csv, already exists.\n",
      "Skipping transArchive_201204_201206_inactive.csv, already exists.\n",
      "Skipping transArchive_201207_201209.csv, already exists.\n",
      "Skipping transArchive_201207_201209_inactive.csv, already exists.\n",
      "Skipping transArchive_201210_201212.csv, already exists.\n",
      "Skipping transArchive_201210_201212_inactive.csv, already exists.\n",
      "Skipping transArchive_201301_201303.csv, already exists.\n",
      "Skipping transArchive_201301_201303_inactive.csv, already exists.\n",
      "Skipping transArchive_201304_201306.csv, already exists.\n",
      "Skipping transArchive_201304_201306_inactive.csv, already exists.\n",
      "Skipping transArchive_201307_201309.csv, already exists.\n",
      "Skipping transArchive_201307_201309_inactive.csv, already exists.\n",
      "Skipping transArchive_201310_201312.csv, already exists.\n",
      "Skipping transArchive_201310_201312_inactive.csv, already exists.\n",
      "Skipping transArchive_201401_201403.csv, already exists.\n",
      "Skipping transArchive_201401_201403_inactive.csv, already exists.\n",
      "Skipping transArchive_201404_201406.csv, already exists.\n",
      "Skipping transArchive_201404_201406_inactive.csv, already exists.\n",
      "Skipping transArchive_201407_201409.csv, already exists.\n",
      "Skipping transArchive_201407_201409_inactive.csv, already exists.\n",
      "Skipping transArchive_201410_201412.csv, already exists.\n",
      "Skipping transArchive_201410_201412_inactive.csv, already exists.\n",
      "Skipping transArchive_201501_201503.csv, already exists.\n",
      "Skipping transArchive_201504_201506.csv, already exists.\n",
      "Skipping transArchive_201507_201509.csv, already exists.\n",
      "Skipping transArchive_201510.csv, already exists.\n",
      "Skipping transArchive_201511.csv, already exists.\n",
      "Skipping transArchive_201512.csv, already exists.\n",
      "Skipping transArchive_201601.csv, already exists.\n",
      "Skipping transArchive_201602.csv, already exists.\n",
      "Skipping transArchive_201603.csv, already exists.\n",
      "Skipping transArchive_201604.csv, already exists.\n",
      "Skipping transArchive_201605.csv, already exists.\n",
      "Skipping transArchive_201606.csv, already exists.\n",
      "Skipping transArchive_201607.csv, already exists.\n",
      "Skipping transArchive_201608.csv, already exists.\n",
      "Skipping transArchive_201609.csv, already exists.\n",
      "Skipping transArchive_201610.csv, already exists.\n",
      "Skipping transArchive_201611.csv, already exists.\n",
      "Skipping transArchive_201612.csv, already exists.\n",
      "Skipping transArchive_201701.csv, already exists.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def extract_csv_from_nested_zips(source_folder, destination_folder):\n",
    "    \"\"\"\n",
    "    Extracts all CSV files from nested zip files within the specified folder and saves them to a single destination folder.\n",
    "    \n",
    "    Parameters:\n",
    "    - source_folder (str): The folder containing the nested zip files.\n",
    "    - destination_folder (str): The folder where the extracted CSV files will be saved.\n",
    "\n",
    "    The function handles:\n",
    "    - Walking through directories to locate and extract CSV files from nested zip files.\n",
    "    - Skipping CSV files that already exist in the destination folder.\n",
    "    - Handling invalid or corrupt zip files gracefully.\n",
    "    \"\"\"\n",
    "    # Ensure the destination folder exists\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "    # Traverse the source folder to locate zip files\n",
    "    for root, dirs, files in os.walk(source_folder):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.zip'):\n",
    "                zip_file_path = os.path.join(root, file_name)\n",
    "                \n",
    "                # Verify if the file is a valid zip archive\n",
    "                try:\n",
    "                    with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "                        # Extract only CSV files from the nested zip archive\n",
    "                        for zip_info in zip_file.infolist():\n",
    "                            if zip_info.filename.endswith('.csv'):\n",
    "                                destination_file_path = os.path.join(destination_folder, zip_info.filename)\n",
    "                                \n",
    "                                # Skip extraction if the CSV file already exists\n",
    "                                if not os.path.exists(destination_file_path):\n",
    "                                    zip_file.extract(zip_info, destination_folder)\n",
    "                                    print(f\"Extracted {zip_info.filename} to {destination_folder}\")\n",
    "                                else:\n",
    "                                    print(f\"Skipping {zip_info.filename}, it already exists.\")\n",
    "                except zipfile.BadZipFile:\n",
    "                    print(f\"Skipping {zip_file_path}, not a valid zip file.\")\n",
    "\n",
    "# Define the input paths\n",
    "source_folder_path = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip'\n",
    "destination_folder_path = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files'  \n",
    "\n",
    "# Extract all CSV files from nested zip files\n",
    "extract_csv_from_nested_zips(source_folder_path, destination_folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Transform  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53 CSV files to process.\n",
      "Skipping already processed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201001_201003.csv\n",
      "Skipping already processed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201004_201006.csv\n",
      "Skipping already processed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201007_201009.csv\n",
      "Skipping already processed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201010_201012.csv\n",
      "Skipping already processed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201101_201103.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201104.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201105.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201106.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201107_201109.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201110_201112.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201201_201203.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_17564\\3982378538.py:23: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, delimiter=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201201_201203_inactive.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201204_201206.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_17564\\3982378538.py:23: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, delimiter=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201204_201206_inactive.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201207_201209.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_17564\\3982378538.py:23: DtypeWarning: Columns (33,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, delimiter=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201207_201209_inactive.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201210_201212.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_17564\\3982378538.py:23: DtypeWarning: Columns (33,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, delimiter=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201210_201212_inactive.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201301_201303.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_17564\\3982378538.py:23: DtypeWarning: Columns (33,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, delimiter=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201301_201303_inactive.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201304_201306.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_17564\\3982378538.py:23: DtypeWarning: Columns (33,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, delimiter=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201304_201306_inactive.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201307_201309.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_17564\\3982378538.py:23: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, delimiter=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201307_201309_inactive.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201310_201312.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201310_201312_inactive.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201401_201403.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_17564\\3982378538.py:23: DtypeWarning: Columns (43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, delimiter=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201401_201403_inactive.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201404_201406.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201404_201406_inactive.csv\n",
      "Error: Could not parse D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201407_201409.csv.\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201407_201409_inactive.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201410_201412.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201410_201412_inactive.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201501_201503.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201504_201506.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201507_201509.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201510.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201511.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201512.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201601.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201602.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201603.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201604.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201605.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201606.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201607.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201608.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201609.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201610.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201611.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201612.csv\n",
      "File cleaned and saved: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201701.csv\n",
      "\n",
      "Saved 52 files to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files:\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201001_201003.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201004_201006.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201007_201009.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201010_201012.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201101_201103.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201104.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201105.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201106.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201107_201109.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201110_201112.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201201_201203.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201201_201203_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201204_201206.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201204_201206_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201207_201209.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201207_201209_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201210_201212.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201210_201212_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201301_201303.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201301_201303_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201304_201306.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201304_201306_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201307_201309.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201307_201309_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201310_201312.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201310_201312_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201401_201403.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201401_201403_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201404_201406.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201404_201406_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201407_201409_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201410_201412.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201410_201412_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201501_201503.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201504_201506.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201507_201509.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201510.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201511.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201512.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201601.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201602.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201603.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201604.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201605.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201606.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201607.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201608.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201609.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201610.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201611.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201612.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\\transArchive_201701.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Path to the reference file containing the correct column headers\n",
    "reference_file_path = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/reference_files/transArchive_201001_201003_clean.csv'\n",
    "\n",
    "# Load the reference file to retrieve the correct column headers\n",
    "df_reference = pd.read_csv(reference_file_path)\n",
    "reference_columns = df_reference.columns.tolist()\n",
    "\n",
    "def clean_and_standardize_csv(input_csv_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Cleans and standardizes the input CSV file, ensuring correct headers and handling NULL values.\n",
    "    Saves the cleaned file to the specified output path.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_csv_path (str): Path to the input CSV file.\n",
    "    - output_csv_path (str): Path to save the cleaned CSV file.\n",
    "    \n",
    "    The function performs:\n",
    "    - Automatic delimiter detection, with fallback to semicolon and comma delimiters if needed.\n",
    "    - Replacement of various representations of NULL with None/NaN.\n",
    "    - Ensuring correct headers by either applying the reference headers or using the first row if applicable.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to load the file with automatic delimiter detection\n",
    "        try:\n",
    "            df = pd.read_csv(input_csv_path, sep=None, engine='python')\n",
    "        except pd.errors.ParserError:\n",
    "            # Fallback to semicolon delimiter\n",
    "            df = pd.read_csv(input_csv_path, delimiter=';')\n",
    "        \n",
    "        # If column count doesn't match the reference, retry with comma delimiter\n",
    "        if df.shape[1] != len(reference_columns):\n",
    "            df = pd.read_csv(input_csv_path, delimiter=',')\n",
    "        \n",
    "        # Replace various NULL representations with None/NaN\n",
    "        df.replace({\"NULL\": None, r\"\\\\N\": None, r\"\\N\": None}, inplace=True)\n",
    "        \n",
    "        # Ensure the correct headers are applied\n",
    "        if list(df.columns) != reference_columns:\n",
    "            # Check if the first row contains headers matching the reference\n",
    "            first_row_as_header = df.iloc[0].tolist()\n",
    "            if set(first_row_as_header) == set(reference_columns):\n",
    "                # Use the first row as headers if it matches\n",
    "                df.columns = first_row_as_header\n",
    "                df = df.iloc[1:].reset_index(drop=True)\n",
    "            else:\n",
    "                # Apply the reference headers directly\n",
    "                df.columns = reference_columns\n",
    "        \n",
    "        # Save the cleaned CSV file\n",
    "        df.to_csv(output_csv_path, index=False, sep=\",\")\n",
    "        print(f\"File cleaned and saved: {output_csv_path}\")\n",
    "    \n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: {input_csv_path} is empty.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error: Could not parse {input_csv_path}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {input_csv_path} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_csv_path}: {e}\")\n",
    "\n",
    "def process_csv_files_in_folder(source_folder, destination_folder):\n",
    "    \"\"\"\n",
    "    Processes all CSV files in the source folder by cleaning and standardizing them before saving to the destination folder.\n",
    "    Skips files that have already been processed.\n",
    "    \n",
    "    Parameters:\n",
    "    - source_folder (str): Folder containing the CSV files to be processed.\n",
    "    - destination_folder (str): Folder to save the cleaned and standardized CSV files.\n",
    "    \"\"\"\n",
    "    # Ensure the destination folder exists\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "    \n",
    "    # Get all CSV files from the source folder\n",
    "    csv_files = glob.glob(f\"{source_folder}/**/*.csv\", recursive=True)\n",
    "    print(f\"Found {len(csv_files)} CSV files to process.\")\n",
    "    \n",
    "    # Process each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        output_file_path = os.path.join(destination_folder, os.path.basename(csv_file))\n",
    "        \n",
    "        # Skip files that have already been processed\n",
    "        if os.path.exists(output_file_path):\n",
    "            print(f\"Skipping already processed file: {output_file_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Clean and save the file\n",
    "        clean_and_standardize_csv(csv_file, output_file_path)\n",
    "    \n",
    "    # List all saved files in the destination folder\n",
    "    saved_files = glob.glob(f\"{destination_folder}/*.csv\")\n",
    "    print(f\"\\nSaved {len(saved_files)} files to {destination_folder}:\")\n",
    "    for saved_file in saved_files:\n",
    "        print(saved_file)\n",
    "\n",
    "# Define the input and output folder paths\n",
    "source_folder_path = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files'\n",
    "destination_folder_path = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files'\n",
    "\n",
    "# Process all CSV files\n",
    "process_csv_files_in_folder(source_folder_path, destination_folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequest",
     "evalue": "400 Error while reading data, error message: CSV processing encountered too many errors, giving up. Rows: 0; errors: 22; max bad: 0; error percent: 0; reason: invalid, message: Error while reading data, error message: CSV processing encountered too many errors, giving up. Rows: 0; errors: 22; max bad: 0; error percent: 0; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 2 byte_offset_to_start_of_line: 22 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 3 byte_offset_to_start_of_line: 42 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 4 byte_offset_to_start_of_line: 53 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 5 byte_offset_to_start_of_line: 61 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 6 byte_offset_to_start_of_line: 76 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 7 byte_offset_to_start_of_line: 86 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 8 byte_offset_to_start_of_line: 96 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 9 byte_offset_to_start_of_line: 105 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 10 byte_offset_to_start_of_line: 113 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 11 byte_offset_to_start_of_line: 126 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 12 byte_offset_to_start_of_line: 142 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 13 byte_offset_to_start_of_line: 160 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 14 byte_offset_to_start_of_line: 177 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 15 byte_offset_to_start_of_line: 186 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 16 byte_offset_to_start_of_line: 200 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 17 byte_offset_to_start_of_line: 213 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 18 byte_offset_to_start_of_line: 230 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 19 byte_offset_to_start_of_line: 244 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 20 byte_offset_to_start_of_line: 256 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 21 byte_offset_to_start_of_line: 275 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 22 byte_offset_to_start_of_line: 294 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 23 byte_offset_to_start_of_line: 307 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 110\u001b[0m\n\u001b[0;32m    107\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Run the function to load CSVs from the specified folder to BigQuery\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m \u001b[43mload_csv_to_bigquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[82], line 102\u001b[0m, in \u001b[0;36mload_csv_to_bigquery\u001b[1;34m(folder_path, dataset_id)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m    101\u001b[0m     load_job \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mload_table_from_file(file, table_ref, job_config\u001b[38;5;241m=\u001b[39mjob_config)\n\u001b[1;32m--> 102\u001b[0m     \u001b[43mload_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait for the job to complete\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\cloud\\bigquery\\job\\base.py:966\u001b[0m, in \u001b[0;36m_AsyncJob.result\u001b[1;34m(self, retry, timeout)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_begin(retry\u001b[38;5;241m=\u001b[39mretry, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    965\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;129;01mis\u001b[39;00m DEFAULT_RETRY \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretry\u001b[39m\u001b[38;5;124m\"\u001b[39m: retry}\n\u001b[1;32m--> 966\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_AsyncJob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\api_core\\future\\polling.py:261\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[1;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking_poll(timeout\u001b[38;5;241m=\u001b[39mtimeout, retry\u001b[38;5;241m=\u001b[39mretry, polling\u001b[38;5;241m=\u001b[39mpolling)\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[1;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "\u001b[1;31mBadRequest\u001b[0m: 400 Error while reading data, error message: CSV processing encountered too many errors, giving up. Rows: 0; errors: 22; max bad: 0; error percent: 0; reason: invalid, message: Error while reading data, error message: CSV processing encountered too many errors, giving up. Rows: 0; errors: 22; max bad: 0; error percent: 0; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 2 byte_offset_to_start_of_line: 22 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 3 byte_offset_to_start_of_line: 42 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 4 byte_offset_to_start_of_line: 53 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 5 byte_offset_to_start_of_line: 61 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 6 byte_offset_to_start_of_line: 76 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 7 byte_offset_to_start_of_line: 86 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 8 byte_offset_to_start_of_line: 96 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 9 byte_offset_to_start_of_line: 105 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 10 byte_offset_to_start_of_line: 113 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 11 byte_offset_to_start_of_line: 126 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 12 byte_offset_to_start_of_line: 142 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 13 byte_offset_to_start_of_line: 160 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 14 byte_offset_to_start_of_line: 177 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 15 byte_offset_to_start_of_line: 186 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 16 byte_offset_to_start_of_line: 200 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 17 byte_offset_to_start_of_line: 213 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 18 byte_offset_to_start_of_line: 230 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 19 byte_offset_to_start_of_line: 244 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 20 byte_offset_to_start_of_line: 256 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 21 byte_offset_to_start_of_line: 275 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 22 byte_offset_to_start_of_line: 294 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE; reason: invalid, message: Error while reading data, error message: CSV table references column position 49, but line contains only 2 columns.; line_number: 23 byte_offset_to_start_of_line: 307 column_index: 49 column_name: \"trans_id\" column_type: DOUBLE"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "\n",
    "# Initialize the BigQuery client with the correct project ID\n",
    "client = bigquery.Client(project='wedgeproject-rileyororke')\n",
    "\n",
    "# Define the dataset ID and create the dataset if it does not exist\n",
    "dataset_id = 'wedgeproject-rileyororke.transaction_tables'\n",
    "dataset_ref = bigquery.Dataset(dataset_id)\n",
    "client.create_dataset(dataset_ref, exists_ok=True)\n",
    "\n",
    "# Schema definition for the transaction tables\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"datetime\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"register_no\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"emp_no\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"trans_no\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"upc\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"description\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"trans_type\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"trans_subtype\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"trans_status\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"department\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"quantity\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"Scale\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"cost\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"unitPrice\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"total\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"regPrice\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"altPrice\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"tax\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"taxexempt\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"foodstamp\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"wicable\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"discount\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"memDiscount\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"discountable\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"discounttype\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"voided\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"percentDiscount\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"ItemQtty\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"volDiscType\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"volume\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"VolSpecial\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"mixMatch\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"matched\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"memType\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"staff\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"numflag\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"itemstatus\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"tenderstatus\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"charflag\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"varflag\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"batchHeaderID\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"local\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"organic\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"display\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"receipt\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"card_no\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"store\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"branch\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"match_id\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"trans_id\", \"FLOAT\"),\n",
    "]\n",
    "\n",
    "def load_csv_to_bigquery(folder_path, dataset_id):\n",
    "    \"\"\"\n",
    "    Load all CSV files from the specified folder into BigQuery.\n",
    "    Each file is uploaded to a table named after the file (without extension).\n",
    "    Skips tables that already exist in BigQuery.\n",
    "    \n",
    "    Parameters:\n",
    "    - folder_path (str): The folder containing the CSV files to be uploaded.\n",
    "    - dataset_id (str): The dataset in BigQuery where the tables will be stored.\n",
    "    \"\"\"\n",
    "    # Iterate over all files in the folder and process only CSV files\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Use the file name (without extension) as the table ID\n",
    "            table_id = os.path.splitext(file_name)[0]\n",
    "            \n",
    "            # Define the table reference within the dataset\n",
    "            table_ref = client.dataset('transaction_tables').table(table_id)\n",
    "\n",
    "            # Check if the table already exists\n",
    "            try:\n",
    "                client.get_table(table_ref)\n",
    "                print(f\"Skipping table {table_id}, it already exists.\")\n",
    "                continue  # Skip if the table exists\n",
    "            except Exception:\n",
    "                # If the table does not exist, proceed with loading\n",
    "                pass\n",
    "\n",
    "            # Configure the load job for CSV format\n",
    "            job_config = bigquery.LoadJobConfig(\n",
    "                source_format=bigquery.SourceFormat.CSV,\n",
    "                schema=schema,\n",
    "                skip_leading_rows=1  # Skip header row\n",
    "            )\n",
    "\n",
    "            # Open the CSV file and load it into BigQuery\n",
    "            with open(file_path, 'rb') as csv_file:\n",
    "                load_job = client.load_table_from_file(csv_file, table_ref, job_config=job_config)\n",
    "                load_job.result()  # Wait for the job to complete\n",
    "            \n",
    "            print(f\"Loaded {file_name} into {dataset_id}.{table_id}\")\n",
    "\n",
    "# Define the folder path for the cleaned CSV files\n",
    "folder_path = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/final_cleaned_csv_files'\n",
    "\n",
    "# Run the function to load CSVs from the folder to BigQuery\n",
    "load_csv_to_bigquery(folder_path, dataset_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
