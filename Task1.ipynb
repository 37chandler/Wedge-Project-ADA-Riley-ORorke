{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Extract the Main Zip File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping transArchive_201001_201003.zip, already exists.\n",
      "Skipping transArchive_201004_201006.zip, already exists.\n",
      "Skipping transArchive_201007_201009.zip, already exists.\n",
      "Skipping transArchive_201010_201012.zip, already exists.\n",
      "Skipping transArchive_201101_201103.zip, already exists.\n",
      "Skipping transArchive_201104.zip, already exists.\n",
      "Skipping transArchive_201105.zip, already exists.\n",
      "Skipping transArchive_201106.zip, already exists.\n",
      "Skipping transArchive_201107_201109.zip, already exists.\n",
      "Skipping transArchive_201110_201112.zip, already exists.\n",
      "Skipping transArchive_201201_201203.zip, already exists.\n",
      "Extracted transArchive_201201_201203_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201201_201203_inactive.zip\n",
      "Skipping transArchive_201204_201206.zip, already exists.\n",
      "Extracted transArchive_201204_201206_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201204_201206_inactive.zip\n",
      "Skipping transArchive_201207_201209.zip, already exists.\n",
      "Extracted transArchive_201207_201209_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201207_201209_inactive.zip\n",
      "Skipping transArchive_201210_201212.zip, already exists.\n",
      "Extracted transArchive_201210_201212_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201210_201212_inactive.zip\n",
      "Skipping transArchive_201301_201303.zip, already exists.\n",
      "Extracted transArchive_201301_201303_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201301_201303_inactive.zip\n",
      "Skipping transArchive_201304_201306.zip, already exists.\n",
      "Extracted transArchive_201304_201306_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201304_201306_inactive.zip\n",
      "Skipping transArchive_201307_201309.zip, already exists.\n",
      "Extracted transArchive_201307_201309_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201307_201309_inactive.zip\n",
      "Skipping transArchive_201310_201312.zip, already exists.\n",
      "Extracted transArchive_201310_201312_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201310_201312_inactive.zip\n",
      "Skipping transArchive_201401_201403.zip, already exists.\n",
      "Extracted transArchive_201401_201403_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201401_201403_inactive.zip\n",
      "Skipping transArchive_201404_201406.zip, already exists.\n",
      "Extracted transArchive_201404_201406_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201404_201406_inactive.zip\n",
      "Skipping transArchive_201407_201409.zip, already exists.\n",
      "Extracted transArchive_201407_201409_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201407_201409_inactive.zip\n",
      "Skipping transArchive_201410_201412.zip, already exists.\n",
      "Extracted transArchive_201410_201412_inactive.zip to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip\\transArchive_201410_201412_inactive.zip\n",
      "Skipping transArchive_201501_201503.zip, already exists.\n",
      "Skipping transArchive_201504_201506.zip, already exists.\n",
      "Skipping transArchive_201507_201509.zip, already exists.\n",
      "Skipping transArchive_201510.zip, already exists.\n",
      "Skipping transArchive_201511.zip, already exists.\n",
      "Skipping transArchive_201512.zip, already exists.\n",
      "Skipping transArchive_201601.zip, already exists.\n",
      "Skipping transArchive_201602.zip, already exists.\n",
      "Skipping transArchive_201603.zip, already exists.\n",
      "Skipping transArchive_201604.zip, already exists.\n",
      "Skipping transArchive_201605.zip, already exists.\n",
      "Skipping transArchive_201606.zip, already exists.\n",
      "Skipping transArchive_201607.zip, already exists.\n",
      "Skipping transArchive_201608.zip, already exists.\n",
      "Skipping transArchive_201609.zip, already exists.\n",
      "Skipping transArchive_201610.zip, already exists.\n",
      "Skipping transArchive_201611.zip, already exists.\n",
      "Skipping transArchive_201612.zip, already exists.\n",
      "Skipping transArchive_201701.zip, already exists.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def extract_main_zip(main_zip_file, extract_to_folder):\n",
    "    # Ensure the extraction folder exists\n",
    "    os.makedirs(extract_to_folder, exist_ok=True)\n",
    "\n",
    "    # Open the main zip file\n",
    "    with zipfile.ZipFile(main_zip_file, 'r') as main_zip:\n",
    "        # Loop through all files in the main zip file\n",
    "        for zip_info in main_zip.infolist():\n",
    "            # Create the full output path\n",
    "            output_file_path = os.path.join(extract_to_folder, zip_info.filename)\n",
    "            \n",
    "            # Check if the file or folder already exists\n",
    "            if os.path.exists(output_file_path):\n",
    "                print(f\"Skipping {zip_info.filename}, already exists.\")\n",
    "                continue\n",
    "\n",
    "            # Check if it's a directory, then create it without extraction\n",
    "            if zip_info.is_dir():\n",
    "                os.makedirs(output_file_path, exist_ok=True)\n",
    "                print(f\"Created directory {output_file_path}\")\n",
    "            else:\n",
    "                # Create any necessary directories\n",
    "                os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "                \n",
    "                # Extract the file\n",
    "                with main_zip.open(zip_info) as source, open(output_file_path, 'wb') as target:\n",
    "                    shutil.copyfileobj(source, target)\n",
    "                print(f\"Extracted {zip_info.filename} to {output_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "main_zip = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/WedgeZipOfZips.zip'\n",
    "extract_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip'\n",
    "\n",
    "extract_main_zip(main_zip, extract_folder)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Extract the Nested Zip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping transArchive_201001_201003.csv, already exists.\n",
      "Skipping transArchive_201004_201006.csv, already exists.\n",
      "Skipping transArchive_201007_201009.csv, already exists.\n",
      "Skipping transArchive_201010_201012.csv, already exists.\n",
      "Skipping transArchive_201101_201103.csv, already exists.\n",
      "Skipping transArchive_201104.csv, already exists.\n",
      "Skipping transArchive_201105.csv, already exists.\n",
      "Skipping transArchive_201106.csv, already exists.\n",
      "Skipping transArchive_201107_201109.csv, already exists.\n",
      "Skipping transArchive_201110_201112.csv, already exists.\n",
      "Skipping transArchive_201201_201203.csv, already exists.\n",
      "Extracted transArchive_201201_201203_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201204_201206.csv, already exists.\n",
      "Extracted transArchive_201204_201206_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201207_201209.csv, already exists.\n",
      "Extracted transArchive_201207_201209_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201210_201212.csv, already exists.\n",
      "Extracted transArchive_201210_201212_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201301_201303.csv, already exists.\n",
      "Extracted transArchive_201301_201303_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201304_201306.csv, already exists.\n",
      "Extracted transArchive_201304_201306_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201307_201309.csv, already exists.\n",
      "Extracted transArchive_201307_201309_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201310_201312.csv, already exists.\n",
      "Extracted transArchive_201310_201312_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201401_201403.csv, already exists.\n",
      "Extracted transArchive_201401_201403_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201404_201406.csv, already exists.\n",
      "Extracted transArchive_201404_201406_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201407_201409.csv, already exists.\n",
      "Extracted transArchive_201407_201409_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201410_201412.csv, already exists.\n",
      "Extracted transArchive_201410_201412_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\n",
      "Skipping transArchive_201501_201503.csv, already exists.\n",
      "Skipping transArchive_201504_201506.csv, already exists.\n",
      "Skipping transArchive_201507_201509.csv, already exists.\n",
      "Skipping transArchive_201510.csv, already exists.\n",
      "Skipping transArchive_201511.csv, already exists.\n",
      "Skipping transArchive_201512.csv, already exists.\n",
      "Skipping transArchive_201601.csv, already exists.\n",
      "Skipping transArchive_201602.csv, already exists.\n",
      "Skipping transArchive_201603.csv, already exists.\n",
      "Skipping transArchive_201604.csv, already exists.\n",
      "Skipping transArchive_201605.csv, already exists.\n",
      "Skipping transArchive_201606.csv, already exists.\n",
      "Skipping transArchive_201607.csv, already exists.\n",
      "Skipping transArchive_201608.csv, already exists.\n",
      "Skipping transArchive_201609.csv, already exists.\n",
      "Skipping transArchive_201610.csv, already exists.\n",
      "Skipping transArchive_201611.csv, already exists.\n",
      "Skipping transArchive_201612.csv, already exists.\n",
      "Skipping transArchive_201701.csv, already exists.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def extract_all_csvs_to_one_folder(extract_folder, output_folder):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Walk through the extracted folder and look for zip files\n",
    "    for root, dirs, files in os.walk(extract_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.zip'):\n",
    "                nested_zip_path = os.path.join(root, file)\n",
    "                \n",
    "                # Check if the file is a valid zip file before proceeding\n",
    "                try:\n",
    "                    with zipfile.ZipFile(nested_zip_path, 'r') as nested_zip:\n",
    "                        for zip_info in nested_zip.infolist():\n",
    "                            if zip_info.filename.endswith('.csv'):\n",
    "                                output_file_path = os.path.join(output_folder, zip_info.filename)\n",
    "                                # Check if the CSV file already exists in the output folder\n",
    "                                if not os.path.exists(output_file_path):\n",
    "                                    # Extract the CSV if it doesn't already exist\n",
    "                                    nested_zip.extract(zip_info, output_folder)\n",
    "                                    print(f\"Extracted {zip_info.filename} to {output_folder}\")\n",
    "                                else:\n",
    "                                    print(f\"Skipping {zip_info.filename}, already exists.\")\n",
    "                except zipfile.BadZipFile:\n",
    "                    print(f\"Skipping {nested_zip_path}, not a valid zip file.\")\n",
    "\n",
    "# Example usage\n",
    "extract_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_main_zip'\n",
    "output_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files'  \n",
    "\n",
    "extract_all_csvs_to_one_folder(extract_folder, output_folder)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Standardize the CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53 CSV files to process.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201001_201003.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201004_201006.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201007_201009.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201010_201012.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201101_201103.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201104.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201105.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201106.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201107_201109.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201110_201112.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201201_201203.csv, already processed.\n",
      "Processing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201201_201203_inactive.csv\n",
      "Successfully read D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201201_201203_inactive.csv\n",
      "Standardized and saved D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201201_201203_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201201_201203_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201204_201206.csv, already processed.\n",
      "Processing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201204_201206_inactive.csv\n",
      "Successfully read D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201204_201206_inactive.csv\n",
      "Standardized and saved D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201204_201206_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201204_201206_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201207_201209.csv, already processed.\n",
      "Processing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201207_201209_inactive.csv\n",
      "Successfully read D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201207_201209_inactive.csv\n",
      "Standardized and saved D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201207_201209_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201207_201209_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201210_201212.csv, already processed.\n",
      "Processing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201210_201212_inactive.csv\n",
      "Successfully read D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201210_201212_inactive.csv\n",
      "Standardized and saved D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201210_201212_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201210_201212_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201301_201303.csv, already processed.\n",
      "Processing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201301_201303_inactive.csv\n",
      "Successfully read D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201301_201303_inactive.csv\n",
      "Standardized and saved D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201301_201303_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201301_201303_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201304_201306.csv, already processed.\n",
      "Processing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201304_201306_inactive.csv\n",
      "Successfully read D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201304_201306_inactive.csv\n",
      "Standardized and saved D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201304_201306_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201304_201306_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201307_201309.csv, already processed.\n",
      "Processing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201307_201309_inactive.csv\n",
      "Successfully read D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201307_201309_inactive.csv\n",
      "Standardized and saved D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201307_201309_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201307_201309_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201310_201312.csv, already processed.\n",
      "Processing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201310_201312_inactive.csv\n",
      "Successfully read D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201310_201312_inactive.csv\n",
      "Standardized and saved D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201310_201312_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201310_201312_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201401_201403.csv, already processed.\n",
      "Processing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201401_201403_inactive.csv\n",
      "Successfully read D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201401_201403_inactive.csv\n",
      "Standardized and saved D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201401_201403_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201401_201403_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201404_201406.csv, already processed.\n",
      "Processing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201404_201406_inactive.csv\n",
      "Successfully read D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201404_201406_inactive.csv\n",
      "Standardized and saved D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201404_201406_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201404_201406_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201407_201409.csv, already processed.\n",
      "Processing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201407_201409_inactive.csv\n",
      "Successfully read D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201407_201409_inactive.csv\n",
      "Standardized and saved D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201407_201409_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201407_201409_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201410_201412.csv, already processed.\n",
      "Processing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201410_201412_inactive.csv\n",
      "Successfully read D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201410_201412_inactive.csv\n",
      "Standardized and saved D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201410_201412_inactive.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201410_201412_inactive.csv\n",
      "Processing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201501_201503.csv\n",
      "Successfully read D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201501_201503.csv\n",
      "Standardized and saved D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201501_201503.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201501_201503.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201504_201506.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201507_201509.csv, already processed.\n",
      "Processing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201510.csv\n",
      "Successfully read D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201510.csv\n",
      "Standardized and saved D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201510.csv to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201510.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201511.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201512.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201601.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201602.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201603.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201604.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201605.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201606.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201607.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201608.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201609.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201610.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201611.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201612.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files\\transArchive_201701.csv, already processed.\n",
      "\n",
      "Saved 53 files to D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files:\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201001_201003.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201004_201006.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201007_201009.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201010_201012.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201101_201103.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201104.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201105.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201106.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201107_201109.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201110_201112.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201201_201203.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201201_201203_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201204_201206.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201204_201206_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201207_201209.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201207_201209_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201210_201212.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201210_201212_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201301_201303.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201301_201303_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201304_201306.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201304_201306_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201307_201309.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201307_201309_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201310_201312.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201310_201312_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201401_201403.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201401_201403_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201404_201406.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201404_201406_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201407_201409.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201407_201409_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201410_201412.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201410_201412_inactive.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201501_201503.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201504_201506.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201507_201509.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201510.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201511.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201512.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201601.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201602.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201603.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201604.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201605.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201606.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201607.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201608.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201609.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201610.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201611.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201612.csv\n",
      "D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201701.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def clean_and_standardize_file(input_file, output_file):\n",
    "    try:\n",
    "        print(f\"Processing file: {input_file}\")\n",
    "        \n",
    "        # Read the CSV file, handling any bad lines and issues with quoting\n",
    "        df = pd.read_csv(\n",
    "            input_file,\n",
    "            sep=None,  # Automatically detect delimiter\n",
    "            engine='python',\n",
    "            on_bad_lines='skip',  # Skip problematic lines\n",
    "            quoting=3  # Treat all quotes as regular characters (disable quoting issues)\n",
    "        )\n",
    "        print(f\"Successfully read {input_file}\")\n",
    "        \n",
    "        # Replace different forms of NULL values with None/NaN\n",
    "        df.replace({\"NULL\": None, r\"\\\\N\": None, r\"\\N\": None}, inplace=True)\n",
    "        \n",
    "        # Save the cleaned file with a standard delimiter (comma)\n",
    "        df.to_csv(output_file, index=False, sep=\",\")\n",
    "        print(f\"Standardized and saved {input_file} to {output_file}\")\n",
    "    \n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: {input_file} is empty.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {input_file} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_file}: {e}\")\n",
    "\n",
    "def process_extracted_csvs(extracted_folder, output_folder):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get all CSV files in the extracted folder\n",
    "    csv_files = glob.glob(f\"{extracted_folder}/**/*.csv\", recursive=True)\n",
    "    print(f\"Found {len(csv_files)} CSV files to process.\")\n",
    "    \n",
    "    # Process each file one by one\n",
    "    for csv_file in csv_files:\n",
    "        output_file = os.path.join(output_folder, os.path.basename(csv_file))\n",
    "        \n",
    "        # Check if the output file already exists (skip if already processed)\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Skipping {csv_file}, already processed.\")\n",
    "            continue\n",
    "        \n",
    "        # Clean and standardize the file\n",
    "        clean_and_standardize_file(csv_file, output_file)\n",
    "    \n",
    "    # After processing, list all files in the output directory\n",
    "    saved_files = glob.glob(f\"{output_folder}/*.csv\")\n",
    "    print(f\"\\nSaved {len(saved_files)} files to {output_folder}:\")\n",
    "    for file in saved_files:\n",
    "        print(file)\n",
    "\n",
    "# Example usage\n",
    "extracted_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/extracted_csv_files'  # Folder where your extracted CSVs are located\n",
    "output_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files'  # Folder to save cleaned CSVs\n",
    "\n",
    "# Process the full files instead of samples\n",
    "process_extracted_csvs(extracted_folder, output_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check delimiter and Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201001_201003.csv\n",
      "File loaded successfully with comma delimiter.\n",
      "No unstandardized null values found.\n",
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201004_201006.csv\n",
      "File loaded successfully with comma delimiter.\n",
      "No unstandardized null values found.\n",
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201007_201009.csv\n",
      "File loaded successfully with comma delimiter.\n",
      "No unstandardized null values found.\n",
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201010_201012.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2178789332.py:13: DtypeWarning: Columns (43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file, sep=\",\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully with comma delimiter.\n",
      "No unstandardized null values found.\n",
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201101_201103.csv\n",
      "File loaded successfully with comma delimiter.\n",
      "No unstandardized null values found.\n",
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201104.csv\n",
      "File loaded successfully with comma delimiter.\n",
      "No unstandardized null values found.\n",
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201105.csv\n",
      "File loaded successfully with comma delimiter.\n",
      "No unstandardized null values found.\n",
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201106.csv\n",
      "File loaded successfully with comma delimiter.\n",
      "No unstandardized null values found.\n",
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201107_201109.csv\n",
      "File loaded successfully with comma delimiter.\n",
      "No unstandardized null values found.\n",
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201110_201112.csv\n",
      "File loaded successfully with comma delimiter.\n",
      "No unstandardized null values found.\n",
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201201_201203.csv\n",
      "File loaded successfully with comma delimiter.\n",
      "No unstandardized null values found.\n",
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201201_201203_inactive.csv\n",
      "File loaded successfully with comma delimiter.\n",
      "No unstandardized null values found.\n",
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201204_201206.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2178789332.py:13: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file, sep=\",\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully with comma delimiter.\n",
      "No unstandardized null values found.\n",
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201204_201206_inactive.csv\n",
      "File loaded successfully with comma delimiter.\n",
      "No unstandardized null values found.\n",
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201207_201209.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2178789332.py:13: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file, sep=\",\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully with comma delimiter.\n",
      "No unstandardized null values found.\n",
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201207_201209_inactive.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2178789332.py:13: DtypeWarning: Columns (33,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file, sep=\",\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully with comma delimiter.\n",
      "No unstandardized null values found.\n",
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201210_201212.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2178789332.py:13: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file, sep=\",\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully with comma delimiter.\n",
      "No unstandardized null values found.\n",
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201210_201212_inactive.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2178789332.py:13: DtypeWarning: Columns (33,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file, sep=\",\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully with comma delimiter.\n",
      "No unstandardized null values found.\n",
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201301_201303.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2178789332.py:13: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file, sep=\",\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully with comma delimiter.\n",
      "No unstandardized null values found.\n",
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201301_201303_inactive.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2178789332.py:13: DtypeWarning: Columns (33,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file, sep=\",\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully with comma delimiter.\n",
      "No unstandardized null values found.\n",
      "\n",
      "Checking file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201304_201306.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     27\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mcheck_delimiters_and_nulls\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m, in \u001b[0;36mcheck_delimiters_and_nulls\u001b[1;34m(output_folder)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Check if the file can be loaded with a comma as the delimiter\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mChecking file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile loaded successfully with comma delimiter.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Check for unstandardized null values (e.g., \"NULL\", \"\\N\", \"\\\\N\")\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:236\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    234\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_concatenate_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:376\u001b[0m, in \u001b[0;36m_concatenate_chunks\u001b[1;34m(chunks)\u001b[0m\n\u001b[0;32m    374\u001b[0m     result[name] \u001b[38;5;241m=\u001b[39m union_categoricals(arrs, sort_categories\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 376\u001b[0m     result[name] \u001b[38;5;241m=\u001b[39m \u001b[43mconcat_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(non_cat_dtypes) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m result[name]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;28mobject\u001b[39m):\n\u001b[0;32m    378\u001b[0m         warning_columns\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(name))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\dtypes\\concat.py:78\u001b[0m, in \u001b[0;36mconcat_compat\u001b[1;34m(to_concat, axis, ea_compat_axis)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     77\u001b[0m     to_concat_arrs \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence[np.ndarray]\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_concat)\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat_arrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m to_concat_eas \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence[ExtensionArray]\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_concat)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ea_compat_axis:\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# We have 1D objects, that don't support axis keyword\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Delimiter check and null check\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def check_delimiters_and_nulls(output_folder):\n",
    "    # Get all CSV files in the output folder\n",
    "    csv_files = glob.glob(f\"{output_folder}/*.csv\")\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            # Check if the file can be loaded with a comma as the delimiter\n",
    "            print(f\"\\nChecking file: {csv_file}\")\n",
    "            df = pd.read_csv(csv_file, sep=\",\")\n",
    "            print(f\"File loaded successfully with comma delimiter.\")\n",
    "            \n",
    "            # Check for unstandardized null values (e.g., \"NULL\", \"\\N\", \"\\\\N\")\n",
    "            unstandardized_nulls = df.isin([\"NULL\", r\"\\N\", r\"\\\\N\"]).sum().sum()\n",
    "            if unstandardized_nulls == 0:\n",
    "                print(\"No unstandardized null values found.\")\n",
    "            else:\n",
    "                print(f\"Found {unstandardized_nulls} instances of unstandardized null values.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {csv_file}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "output_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files'\n",
    "check_delimiters_and_nulls(output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Collumn Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53 CSV files to process.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201001_201003.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201004_201006.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201007_201009.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201010_201012.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201101_201103.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201104.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201105.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201106.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201107_201109.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201110_201112.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201201_201203.csv, already processed.\n",
      "Fixing headers for file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201201_201203_inactive.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2096642854.py:26: DtypeWarning: Columns (33,34,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201201_201203_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201204_201206.csv, already processed.\n",
      "Fixing headers for file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201204_201206_inactive.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2096642854.py:26: DtypeWarning: Columns (33,34,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201204_201206_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201207_201209.csv, already processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2096642854.py:17: DtypeWarning: Columns (33,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing headers for file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201207_201209_inactive.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2096642854.py:26: DtypeWarning: Columns (33,34,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201207_201209_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201210_201212.csv, already processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2096642854.py:17: DtypeWarning: Columns (33,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing headers for file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201210_201212_inactive.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2096642854.py:26: DtypeWarning: Columns (33,34,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201210_201212_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201301_201303.csv, already processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2096642854.py:17: DtypeWarning: Columns (33,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing headers for file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201301_201303_inactive.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2096642854.py:26: DtypeWarning: Columns (33,34,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201301_201303_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201304_201306.csv, already processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2096642854.py:17: DtypeWarning: Columns (33,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing headers for file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201304_201306_inactive.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2096642854.py:26: DtypeWarning: Columns (33,34,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201304_201306_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201307_201309.csv, already processed.\n",
      "Fixing headers for file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201307_201309_inactive.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2096642854.py:26: DtypeWarning: Columns (33,34,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201307_201309_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201310_201312.csv, already processed.\n",
      "Fixing headers for file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201310_201312_inactive.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2096642854.py:26: DtypeWarning: Columns (33,34,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201310_201312_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201401_201403.csv, already processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2096642854.py:17: DtypeWarning: Columns (34,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing headers for file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201401_201403_inactive.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2096642854.py:26: DtypeWarning: Columns (33,34,40,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201401_201403_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201404_201406.csv, already processed.\n",
      "Fixing headers for file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201404_201406_inactive.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2096642854.py:26: DtypeWarning: Columns (33,34,40,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201404_201406_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201407_201409.csv, already processed.\n",
      "Fixing headers for file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201407_201409_inactive.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2096642854.py:26: DtypeWarning: Columns (33,34,40,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201407_201409_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201410_201412.csv, already processed.\n",
      "Fixing headers for file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201410_201412_inactive.csv\n",
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201410_201412_inactive.csv\n",
      "Fixing headers for file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201501_201503.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2096642854.py:26: DtypeWarning: Columns (16,20,34,39,40,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201501_201503.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201504_201506.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201507_201509.csv, already processed.\n",
      "Fixing headers for file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201510.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Temp\\ipykernel_21868\\2096642854.py:26: DtypeWarning: Columns (16,20,34,39,40,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201510.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201511.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201512.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201601.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201602.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201603.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201604.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201605.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201606.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201607.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201608.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201609.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201610.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201611.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201612.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files\\transArchive_201701.csv, already processed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def get_reference_headers(reference_file):\n",
    "    \"\"\"Reads the reference file to get the correct headers.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(reference_file, nrows=0)  # Only read the header\n",
    "        return list(df.columns)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading reference file: {e}\")\n",
    "        return []\n",
    "\n",
    "def fix_headers_in_file(input_file, output_file, correct_headers):\n",
    "    \"\"\"Checks and fixes the headers of the given file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "\n",
    "        # Compare current headers to reference headers\n",
    "        current_headers = list(df.columns)\n",
    "\n",
    "        if current_headers != correct_headers:\n",
    "            print(f\"Fixing headers for file: {input_file}\")\n",
    "            \n",
    "            # Shift the data down by one row\n",
    "            df = pd.read_csv(input_file, header=None)\n",
    "            df.columns = correct_headers  # Set the correct headers\n",
    "\n",
    "        # Save the corrected file\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved fixed file: {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_file}: {e}\")\n",
    "\n",
    "def process_csv_files(input_folder, output_folder, reference_file):\n",
    "    \"\"\"Iterates over all CSV files and fixes headers if needed.\"\"\"\n",
    "    # Get the correct headers from the reference file\n",
    "    correct_headers = get_reference_headers(reference_file)\n",
    "\n",
    "    if not correct_headers:\n",
    "        print(\"No valid reference headers found, aborting.\")\n",
    "        return\n",
    "\n",
    "    # Get all CSV files in the input folder\n",
    "    csv_files = glob.glob(os.path.join(input_folder, '*.csv'))\n",
    "\n",
    "    print(f\"Found {len(csv_files)} CSV files to process.\")\n",
    "\n",
    "    # Iterate over each CSV file and fix the headers\n",
    "    for csv_file in csv_files:\n",
    "        output_file = os.path.join(output_folder, os.path.basename(csv_file))\n",
    "\n",
    "        # Check if the file has already been processed\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Skipping {csv_file}, already processed.\")\n",
    "            continue\n",
    "        \n",
    "        # Clean and fix headers\n",
    "        fix_headers_in_file(csv_file, output_file, correct_headers)\n",
    "\n",
    "# Example usage\n",
    "input_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files'\n",
    "output_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files'\n",
    "reference_file = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/standardized_csv_files/transArchive_201001_201003.csv'  # Path to the reference CSV\n",
    "\n",
    "# Process the files\n",
    "process_csv_files(input_folder, output_folder, reference_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Column Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53 CSV files to check.\n",
      "Setting reference columns from: transArchive_201001_201003.csv\n",
      "transArchive_201004_201006.csv has consistent columns.\n",
      "transArchive_201007_201009.csv has consistent columns.\n",
      "transArchive_201010_201012.csv has consistent columns.\n",
      "transArchive_201101_201103.csv has consistent columns.\n",
      "transArchive_201104.csv has consistent columns.\n",
      "transArchive_201105.csv has consistent columns.\n",
      "transArchive_201106.csv has consistent columns.\n",
      "transArchive_201107_201109.csv has consistent columns.\n",
      "transArchive_201110_201112.csv has consistent columns.\n",
      "transArchive_201201_201203.csv has consistent columns.\n",
      "transArchive_201201_201203_inactive.csv has consistent columns.\n",
      "transArchive_201204_201206.csv has consistent columns.\n",
      "transArchive_201204_201206_inactive.csv has consistent columns.\n",
      "transArchive_201207_201209.csv has consistent columns.\n",
      "transArchive_201207_201209_inactive.csv has consistent columns.\n",
      "transArchive_201210_201212.csv has consistent columns.\n",
      "transArchive_201210_201212_inactive.csv has consistent columns.\n",
      "transArchive_201301_201303.csv has consistent columns.\n",
      "transArchive_201301_201303_inactive.csv has consistent columns.\n",
      "transArchive_201304_201306.csv has consistent columns.\n",
      "transArchive_201304_201306_inactive.csv has consistent columns.\n",
      "transArchive_201307_201309.csv has consistent columns.\n",
      "transArchive_201307_201309_inactive.csv has consistent columns.\n",
      "transArchive_201310_201312.csv has consistent columns.\n",
      "transArchive_201310_201312_inactive.csv has consistent columns.\n",
      "transArchive_201401_201403.csv has consistent columns.\n",
      "transArchive_201401_201403_inactive.csv has consistent columns.\n",
      "transArchive_201404_201406.csv has consistent columns.\n",
      "transArchive_201404_201406_inactive.csv has consistent columns.\n",
      "transArchive_201407_201409.csv has consistent columns.\n",
      "transArchive_201407_201409_inactive.csv has consistent columns.\n",
      "transArchive_201410_201412.csv has consistent columns.\n",
      "transArchive_201410_201412_inactive.csv has consistent columns.\n",
      "transArchive_201501_201503.csv has consistent columns.\n",
      "transArchive_201504_201506.csv has consistent columns.\n",
      "transArchive_201507_201509.csv has consistent columns.\n",
      "transArchive_201510.csv has consistent columns.\n",
      "transArchive_201511.csv has consistent columns.\n",
      "transArchive_201512.csv has consistent columns.\n",
      "transArchive_201601.csv has consistent columns.\n",
      "transArchive_201602.csv has consistent columns.\n",
      "transArchive_201603.csv has consistent columns.\n",
      "transArchive_201604.csv has consistent columns.\n",
      "transArchive_201605.csv has consistent columns.\n",
      "transArchive_201606.csv has consistent columns.\n",
      "transArchive_201607.csv has consistent columns.\n",
      "transArchive_201608.csv has consistent columns.\n",
      "transArchive_201609.csv has consistent columns.\n",
      "transArchive_201610.csv has consistent columns.\n",
      "transArchive_201611.csv has consistent columns.\n",
      "transArchive_201612.csv has consistent columns.\n",
      "transArchive_201701.csv has consistent columns.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def check_column_names(input_folder):\n",
    "    # Get all CSV files in the input folder\n",
    "    csv_files = glob.glob(os.path.join(input_folder, '*.csv'))\n",
    "    \n",
    "    # Dictionary to store columns for each file\n",
    "    file_columns = {}\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to check.\")\n",
    "\n",
    "    # Variable to store the reference column set (from the first file)\n",
    "    reference_columns = None\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            # Load only the header (first row) to check column names\n",
    "            df = pd.read_csv(csv_file, nrows=0)\n",
    "            columns = list(df.columns)\n",
    "\n",
    "            file_columns[csv_file] = columns\n",
    "\n",
    "            # Compare columns with the reference (first CSV file)\n",
    "            if reference_columns is None:\n",
    "                reference_columns = columns  # Set the first file as the reference\n",
    "                print(f\"Setting reference columns from: {os.path.basename(csv_file)}\")\n",
    "            else:\n",
    "                if columns != reference_columns:\n",
    "                    print(f\"WARNING: {os.path.basename(csv_file)} has different columns!\")\n",
    "                    print(f\"Expected columns: {reference_columns}\")\n",
    "                    print(f\"Found columns: {columns}\")\n",
    "                else:\n",
    "                    print(f\"{os.path.basename(csv_file)} has consistent columns.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {csv_file}: {e}\")\n",
    "\n",
    "    return file_columns\n",
    "\n",
    "# Example usage\n",
    "input_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files'\n",
    "file_columns = check_column_names(input_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing Mismatched Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53 CSV files to process.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201001_201003.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201004_201006.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201007_201009.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201010_201012.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201101_201103.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201104.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201105.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201106.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201107_201109.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201110_201112.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201201_201203.csv, already processed.\n",
      "Fixing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201201_201203_inactive.csv\n",
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201201_201203_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201204_201206.csv, already processed.\n",
      "Fixing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201204_201206_inactive.csv\n",
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201204_201206_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201207_201209.csv, already processed.\n",
      "Fixing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201207_201209_inactive.csv\n",
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201207_201209_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201210_201212.csv, already processed.\n",
      "Fixing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201210_201212_inactive.csv\n",
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201210_201212_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201301_201303.csv, already processed.\n",
      "Fixing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201301_201303_inactive.csv\n",
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201301_201303_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201304_201306.csv, already processed.\n",
      "Fixing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201304_201306_inactive.csv\n",
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201304_201306_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201307_201309.csv, already processed.\n",
      "Fixing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201307_201309_inactive.csv\n",
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201307_201309_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201310_201312.csv, already processed.\n",
      "Fixing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201310_201312_inactive.csv\n",
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201310_201312_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201401_201403.csv, already processed.\n",
      "Fixing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201401_201403_inactive.csv\n",
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201401_201403_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201404_201406.csv, already processed.\n",
      "Fixing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201404_201406_inactive.csv\n",
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201404_201406_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201407_201409.csv, already processed.\n",
      "Fixing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201407_201409_inactive.csv\n",
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201407_201409_inactive.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201410_201412.csv, already processed.\n",
      "Fixing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201410_201412_inactive.csv\n",
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201410_201412_inactive.csv\n",
      "Fixing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201501_201503.csv\n",
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201501_201503.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201504_201506.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201507_201509.csv, already processed.\n",
      "Fixing file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201510.csv\n",
      "Saved fixed file: D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201510.csv\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201511.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201512.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201601.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201602.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201603.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201604.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201605.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201606.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201607.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201608.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201609.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201610.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201611.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201612.csv, already processed.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files\\transArchive_201701.csv, already processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Define the expected data types for each column\n",
    "expected_dtypes = {\n",
    "    \"datetime\": \"datetime64[ns]\",\n",
    "    \"register_no\": \"Int64\",\n",
    "    \"emp_no\": \"Int64\",\n",
    "    \"trans_no\": \"Int64\",\n",
    "    \"upc\": \"string\",\n",
    "    \"description\": \"string\",\n",
    "    \"trans_type\": \"string\",\n",
    "    \"trans_subtype\": \"string\",\n",
    "    \"trans_status\": \"string\",\n",
    "    \"department\": \"Int64\",\n",
    "    \"quantity\": \"float64\",\n",
    "    \"Scale\": \"Int64\",\n",
    "    \"cost\": \"float64\",\n",
    "    \"unitPrice\": \"float64\",\n",
    "    \"total\": \"float64\",\n",
    "    \"regPrice\": \"float64\",\n",
    "    \"altPrice\": \"float64\",\n",
    "    \"tax\": \"Int64\",\n",
    "    \"taxexempt\": \"Int64\",\n",
    "    \"foodstamp\": \"Int64\",\n",
    "    \"wicable\": \"Int64\",\n",
    "    \"discount\": \"float64\",\n",
    "    \"memDiscount\": \"float64\",\n",
    "    \"discountable\": \"Int64\",\n",
    "    \"discounttype\": \"Int64\",\n",
    "    \"voided\": \"Int64\",\n",
    "    \"percentDiscount\": \"float64\",\n",
    "    \"ItemQtty\": \"float64\",\n",
    "    \"volDiscType\": \"Int64\",\n",
    "    \"volume\": \"Int64\",\n",
    "    \"VolSpecial\": \"float64\",\n",
    "    \"mixMatch\": \"Int64\",\n",
    "    \"matched\": \"Int64\",\n",
    "    \"memType\": \"string\",\n",
    "    \"staff\": \"Int64\",\n",
    "    \"numflag\": \"Int64\",\n",
    "    \"itemstatus\": \"Int64\",\n",
    "    \"tenderstatus\": \"Int64\",\n",
    "    \"charflag\": \"string\",\n",
    "    \"varflag\": \"Int64\",\n",
    "    \"batchHeaderID\": \"string\",\n",
    "    \"local\": \"Int64\",\n",
    "    \"organic\": \"string\",\n",
    "    \"display\": \"string\",\n",
    "    \"receipt\": \"Int64\",\n",
    "    \"card_no\": \"Int64\",\n",
    "    \"store\": \"Int64\",\n",
    "    \"branch\": \"Int64\",\n",
    "    \"match_id\": \"Int64\",\n",
    "    \"trans_id\": \"Int64\"\n",
    "}\n",
    "\n",
    "# Function to fix data type mismatches\n",
    "def fix_dtypes(input_file, output_file):\n",
    "    try:\n",
    "        print(f\"Fixing file: {input_file}\")\n",
    "        \n",
    "        # Read the file without specifying data types\n",
    "        df = pd.read_csv(input_file, low_memory=False)\n",
    "        \n",
    "        # Iterate over the expected data types and try to convert columns\n",
    "        for column, expected_type in expected_dtypes.items():\n",
    "            if column in df.columns:\n",
    "                try:\n",
    "                    # Attempt to convert the column to the expected type\n",
    "                    if expected_type == \"datetime64[ns]\":\n",
    "                        df[column] = pd.to_datetime(df[column], format='%m/%d/%Y %H:%M', errors='coerce')  # Convert to datetime\n",
    "                    else:\n",
    "                        df[column] = df[column].astype(expected_type, errors='ignore')  # Convert to expected type\n",
    "                except Exception as e:\n",
    "                    print(f\"Error converting {column} in {input_file}: {e}\")\n",
    "            else:\n",
    "                print(f\"Column '{column}' is missing from {input_file}, skipping conversion.\")\n",
    "\n",
    "        # Save the fixed file\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved fixed file: {output_file}\")\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: {input_file} is empty.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {input_file} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_file}: {e}\")\n",
    "\n",
    "# Process all CSVs to fix data type mismatches\n",
    "def process_and_fix_all_csvs(input_folder, output_folder):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get all CSV files in the input folder\n",
    "    csv_files = glob.glob(os.path.join(input_folder, '*.csv'))\n",
    "    print(f\"Found {len(csv_files)} CSV files to process.\")\n",
    "    \n",
    "    # Fix each file and save the corrected version\n",
    "    for csv_file in csv_files:\n",
    "        # Construct output file path\n",
    "        output_file = os.path.join(output_folder, os.path.basename(csv_file))\n",
    "        \n",
    "        # Check if the file has already been processed\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Skipping {csv_file}, already processed.\")\n",
    "            continue\n",
    "        \n",
    "        # Fix data types and save the corrected file\n",
    "        fix_dtypes(csv_file, output_file)\n",
    "\n",
    "# Example usage\n",
    "input_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleaned_csv_files'  # Folder with standardized CSVs\n",
    "output_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files'  # Folder for cleaned CSVs\n",
    "\n",
    "# Fix all CSVs\n",
    "process_and_fix_all_csvs(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Uploading to Google BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53 CSV files to upload.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201001_201003.csv, table transArchive_201001_201003 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201004_201006.csv, table transArchive_201004_201006 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201007_201009.csv, table transArchive_201007_201009 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201010_201012.csv, table transArchive_201010_201012 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201101_201103.csv, table transArchive_201101_201103 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201104.csv, table transArchive_201104 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201105.csv, table transArchive_201105 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201106.csv, table transArchive_201106 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201107_201109.csv, table transArchive_201107_201109 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201110_201112.csv, table transArchive_201110_201112 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201201_201203.csv, table transArchive_201201_201203 already exists in BigQuery.\n",
      "Loaded 245773 rows into wedgeproject-rileyororke.transaction_tables.transArchive_201201_201203_inactive\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201204_201206.csv, table transArchive_201204_201206 already exists in BigQuery.\n",
      "Loaded 237991 rows into wedgeproject-rileyororke.transaction_tables.transArchive_201204_201206_inactive\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201207_201209.csv, table transArchive_201207_201209 already exists in BigQuery.\n",
      "Loaded 190878 rows into wedgeproject-rileyororke.transaction_tables.transArchive_201207_201209_inactive\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201210_201212.csv, table transArchive_201210_201212 already exists in BigQuery.\n",
      "Loaded 162989 rows into wedgeproject-rileyororke.transaction_tables.transArchive_201210_201212_inactive\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201301_201303.csv, table transArchive_201301_201303 already exists in BigQuery.\n",
      "Loaded 148624 rows into wedgeproject-rileyororke.transaction_tables.transArchive_201301_201303_inactive\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201304_201306.csv, table transArchive_201304_201306 already exists in BigQuery.\n",
      "Loaded 137629 rows into wedgeproject-rileyororke.transaction_tables.transArchive_201304_201306_inactive\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201307_201309.csv, table transArchive_201307_201309 already exists in BigQuery.\n",
      "Loaded 104469 rows into wedgeproject-rileyororke.transaction_tables.transArchive_201307_201309_inactive\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201310_201312.csv, table transArchive_201310_201312 already exists in BigQuery.\n",
      "Loaded 79157 rows into wedgeproject-rileyororke.transaction_tables.transArchive_201310_201312_inactive\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201401_201403.csv, table transArchive_201401_201403 already exists in BigQuery.\n",
      "Loaded 52615 rows into wedgeproject-rileyororke.transaction_tables.transArchive_201401_201403_inactive\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201404_201406.csv, table transArchive_201404_201406 already exists in BigQuery.\n",
      "Loaded 49070 rows into wedgeproject-rileyororke.transaction_tables.transArchive_201404_201406_inactive\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201407_201409.csv, table transArchive_201407_201409 already exists in BigQuery.\n",
      "Loaded 28324 rows into wedgeproject-rileyororke.transaction_tables.transArchive_201407_201409_inactive\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201410_201412.csv, table transArchive_201410_201412 already exists in BigQuery.\n",
      "Loaded 7965 rows into wedgeproject-rileyororke.transaction_tables.transArchive_201410_201412_inactive\n",
      "Loaded 3041130 rows into wedgeproject-rileyororke.transaction_tables.transArchive_201501_201503\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201504_201506.csv, table transArchive_201504_201506 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201507_201509.csv, table transArchive_201507_201509 already exists in BigQuery.\n",
      "Loaded 1006056 rows into wedgeproject-rileyororke.transaction_tables.transArchive_201510\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201511.csv, table transArchive_201511 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201512.csv, table transArchive_201512 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201601.csv, table transArchive_201601 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201602.csv, table transArchive_201602 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201603.csv, table transArchive_201603 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201604.csv, table transArchive_201604 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201605.csv, table transArchive_201605 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201606.csv, table transArchive_201606 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201607.csv, table transArchive_201607 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201608.csv, table transArchive_201608 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201609.csv, table transArchive_201609 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201610.csv, table transArchive_201610 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201611.csv, table transArchive_201611 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201612.csv, table transArchive_201612 already exists in BigQuery.\n",
      "Skipping D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files\\transArchive_201701.csv, table transArchive_201701 already exists in BigQuery.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from google.cloud import bigquery\n",
    "\n",
    "def table_exists(client, dataset_id, table_name):\n",
    "    \"\"\"Check if a table already exists in BigQuery.\"\"\"\n",
    "    try:\n",
    "        client.get_table(f'{dataset_id}.{table_name}')\n",
    "        return True\n",
    "    except Exception:\n",
    "        # Table does not exist\n",
    "        return False\n",
    "\n",
    "def upload_csv_to_bigquery(client, dataset_id, table_name, csv_file):\n",
    "    try:\n",
    "        # Configure the load job with schema autodetection\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format=bigquery.SourceFormat.CSV,\n",
    "            skip_leading_rows=1,  # Skipping header row if the CSV contains headers\n",
    "            autodetect=True  # Automatically detect schema\n",
    "        )\n",
    "\n",
    "        # Load data from CSV into BigQuery\n",
    "        with open(csv_file, \"rb\") as source_file:\n",
    "            load_job = client.load_table_from_file(source_file, f'{dataset_id}.{table_name}', job_config=job_config)\n",
    "\n",
    "        # Wait for the load job to complete\n",
    "        load_job.result()\n",
    "\n",
    "        print(f\"Loaded {load_job.output_rows} rows into {dataset_id}.{table_name}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading {csv_file} to BigQuery: {e}\")\n",
    "\n",
    "def upload_all_csvs(input_folder, dataset_id):\n",
    "    # Initialize the BigQuery client\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Get all CSV files in the input folder\n",
    "    csv_files = glob.glob(os.path.join(input_folder, '*.csv'))\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to upload.\")\n",
    "    \n",
    "    # Iterate through each file and upload it\n",
    "    for csv_file in csv_files:\n",
    "        # Extract the base name of the CSV file to use as the table name\n",
    "        table_name = os.path.basename(csv_file).replace('.csv', '')\n",
    "        \n",
    "        # Check if the table already exists\n",
    "        if table_exists(client, dataset_id, table_name):\n",
    "            print(f\"Skipping {csv_file}, table {table_name} already exists in BigQuery.\")\n",
    "            continue\n",
    "        \n",
    "        # Upload the CSV to BigQuery if the table does not exist\n",
    "        upload_csv_to_bigquery(client, dataset_id, table_name, csv_file)\n",
    "\n",
    "# Example usage\n",
    "input_folder = 'D:/WedgeProject/Wedge-Project-ADA-Riley-ORorke/data/cleanedV2_csv_files'  # Folder with cleaned CSVs\n",
    "dataset_id = 'wedgeproject-rileyororke.transaction_tables'  # Your BigQuery dataset\n",
    "\n",
    "# Upload all CSVs to BigQuery\n",
    "upload_all_csvs(input_folder, dataset_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
