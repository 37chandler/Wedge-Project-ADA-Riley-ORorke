
# Applied Data Analytics

## Wedge Project

The Wedge Project provides hands-on experience in data engineering by working through the stages of Extract, Transform, and Load (ETL), with the goal of extracting raw transaction-level data from the Wedge Co-Op, transforming it into clean, usable formats, and loading it into a structured database for further analysis. The Wedge Co-Op dataset offers a rich source of transactional data spanning years of sales records, but it comes with challenges such as inconsistent formats, null values, and mixed data types. This project walks through the entire ETL process, from cleaning and transforming the raw data to building summary tables for actionable insights. First, Task 1 tackles the ETL process for the data, loading the cleaned files into Google BigQuery. Next, Task 2 focuses on extracting a manageable sample of owner transactions and writes them to a local csv file. Finally, Task 3 summarizes transaction records into analytical tables for fast insights into the data. The project highlights the power of structured ETL pipelines in creating datasets that are easy to analyze and use for further research.

### Task 1

* Files for this task: 
`Task 1 - ETL.ipynb`  

This notebook documents the ETL process for loading raw Wedge Co-Op transaction data into Google BigQuery. The dataset comprises multiple files covering various time periods of transactional data, each presenting challenges such as inconsistent column headers, null values, and unstandardized delimiters.

In the Extract phase, raw transaction data files are retrieved from a local folder, which contains multiple CSV files with varying delimiters, including commas and semicolons. These files hold transactional details such as datetime, register_no, emp_no, upc, description, trans_type, total, and more.

During the Transform phase, the code handles various data cleaning tasks to ensure consistency across all Wedge Co-Op transaction files before loading them into BigQuery. First, it manages inconsistent delimiters by automatically detecting the appropriate delimiter for each file, and when detection fails, it falls back to using semicolons or commas to ensure proper parsing. The code then standardizes the representation of null values, replacing common variations like "NULL," "\\N," and "\N" with Python's `NaN` to maintain uniformity across the dataset. Additionally, it checks and corrects the column headers by comparing them with a reference set of headers from a clean file. If discrepancies are found, the code adjusts the headers, either by correcting them or using the first row as a header if it was mistakenly treated as data. Finally, after transforming the data, the code saves the cleaned files using a consistent comma delimiter to ensure a standardized output format across all files. 

Finally, in the Load phase, the cleaned and standardized data is loaded into Google BigQuery. This process includes enforcing proper data types in BigQuery and verifying the successful loading of records by checking the row counts and schema integrity. This ensures that the entire dataset is ready for further analysis.


### Task 2

* Files for this task: 
`Task 2 - OwnerSample.ipynb`  

This notebook extracts a sample of owner transaction records from the BigQuery dataset for local analysis. The process begins by querying the dataset to retrieve a list of owners, excluding non-owner transactions (card_no == 3). A random sample of 400 owners is selected to create a dataset that is approximately 250 MB in size. This sample is large enough to provide meaningful insights while small enough for local processing.

The transactions associated with the sampled owners are fetched in batches to avoid memory overload, ensuring the extraction process is efficient. These transactions include all relevant purchase details, such as items bought, transaction timestamps, and associated costs. Finally, the extracted data is saved as a local CSV file.
	

### Task 3

* Files for this task: 
`Task 3 - SummaryTables.ipynb`  

In Task 3, the focus is on generating summary tables to provide insights into Wedge Co-Op sales data. First, the Sales by Date by Hour table is generated by aggregating sales data by calendar date and hour, capturing total spend, the number of transactions, and the count of items purchased. Only owner transactions were included, with non-owners excluded using the condition card_no != 3. This allows analyzation of hourly sales patterns at the Co-Op. Next, the Sales by Owner by Year by Month table was generated, summarizing each owner's monthly transactions by calculating total sales, the number of transactions, and items purchased. Grouping the data by year and month enables purchase-trend tracking over time. The third summary table, Sales by Product Description by Year by Month, provided insights into product sales by UPC and description. The table was grouped by year and month to capture sales and transaction counts for each product, and it included department names by joining with the department lookup table.

Once these summaries were created, the data was loaded into an SQLite database (wedge_coop_summary.db) for easy access and analysis. The database contains three tables: sales_by_hour, which captures sales information by date and hour; sales_by_owner, which provides monthly summaries of each owner's transaction activity; and sales_by_product, detailing product sales by department for each month. 

## Query Comparison Results

Fill in the following table with the results from the 
queries contained in `gbq_assessment_query.sql`. You only
need to fill in relative difference on the rows where it applies. 
When calculating relative difference, use the formula 
` (your_results - john_results)/john_results)`. 



|  Query  |  Your Results  |  John's Results | Difference | Rel. Diff | 
|---|---|---|---|---|
| Total Rows  |  81814889 | 85760139  |  3945250 | 4.60 %  |
| January 2012 Rows  |  1070907 |  1070907 |  0 | 0 %  |
| October 2012 Rows  | 1042287  | 1042287  |  0 |  0 % |
| Month with Fewest  |  September |  February | Yes | NA  |
| Num Rows in Month with Fewest  | 5964993  |  6556770|  591777 |  9.03 % |
| Month with Most  |  May | May  | No  | NA  |
| Num Rows in Month with Most  |  7578371|  7578372|  1  | ~ 0 %  |
| Null_TS  |  6626266 | 7123792  |  497526 |  6.98 % |
| Null_DT  |  0 |  0 |  0 |  0 % |
| Null_Local  |224411   | 227054  |  2643 | 1.16 %  |
| Null_CN  | 0  | 0  |  0 |  0 % |
| Num 5 on High Volume Cards  | 14987  |  14987| No  | NA  |
| Num Rows for Number 5 | 439095  |  460630 |  21535 |  4.68 % |
| Num Rows for 18736  | 11761  | 12153 | 392  |  3.23 % |
| Product with Most Rows  | 	banana organic  | banana organic | No  | NA  |
| Num Rows for that Product  | 865574  | 908639| 43065  | 4.74 %  |
| Product with Fourth-Most Rows  |  avocado hass organic |  avocado hass organic | No  | NA  |
| Num Rows for that Product  |  436636 |  456771 | 20135  |  4.41 % |
| Num Single Record Products  |  2814 |  2769 | 45  |  1.63 % |
| Year with Highest Portion of Owner Rows  |  2010 | 2010  | No  | NA |
| Fraction of Rows from Owners in that Year  | 0.7422  |  0.7422 | 0  | 0 %  |
| Year with Lowest Portion of Owner Rows  | 2017| 2017| No  | NA |
| Fraction of Rows from Owners in that Year  |  0.7513 |  0.7513 | 0  | 0 %  |

## Reflections

The Wedge Project was an incredible learning experience that offered deep insights into working with real-world data and utilizing advanced data engineering concepts. The project involved processing a large dataset of transaction records from the Wedge Co-op, one of the largest cooperative grocery stores in the US. Throughout the project, I had to work simultaneously with Python and Google BigQuery to extract, clean, and transform data efficiently while ensuring data integrity and accuracy. One of the key hurdles was implementing automatic delimiter detection and making sure that NULL values were properly standardized across all files. Another challenge was aligning the headers with the reference file, which involved identifying cases where the first row needed to be treated as a header or when headers were entirely missing or misaligned. Overall, this project helped me sharpen my skills in data cleaning, querying, and pipeline automation, providing me with valuable experience for future data engineering tasks.
