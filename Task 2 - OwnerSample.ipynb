{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: A Sample of Owners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview  \n",
    "In Task 2, the goal is to extract a sample of owner transaction records from the Wedge Co-Op dataset in Google BigQuery. This allows for more efficient local analysis by working with a smaller subset of the data. The process involves selecting a random sample of 400 unique owners from the dataset, excluding non-owners (denoted by card_no == 3). All transactions associated with the sampled owners are then retrieved in batches and saved locally as a CSV file. The extracted sample is designed to be around 250MB in size, providing a manageable dataset for analysis while maintaining data richness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\cloud\\bigquery\\table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Riley_26\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\cloud\\bigquery\\table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Riley_26\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\cloud\\bigquery\\table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled transactions extracted and saved to owner_transactions.csv\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define project and dataset\n",
    "project_id = \"umt-msba\"\n",
    "dataset_id = \"transactions\"\n",
    "\n",
    "# Define sample size for owner records (400 owners for approximately 250MB)\n",
    "sample_size = 400\n",
    "\n",
    "# Query to sample unique owners, excluding non-owners (card_no == 3)\n",
    "owner_query = f\"\"\"\n",
    "    WITH unique_owners AS (\n",
    "        SELECT DISTINCT card_no\n",
    "        FROM `{project_id}.{dataset_id}.transArchive_*`\n",
    "        WHERE card_no != 3\n",
    "    )\n",
    "    SELECT card_no\n",
    "    FROM unique_owners\n",
    "    ORDER BY RAND()\n",
    "    LIMIT {sample_size}\n",
    "\"\"\"\n",
    "# Execute the query and load sampled owner data into a DataFrame\n",
    "sampled_owners_df = client.query(owner_query).to_dataframe()\n",
    "\n",
    "# Convert the sampled owners to a list of card_no values\n",
    "owner_list = sampled_owners_df['card_no'].tolist()\n",
    "\n",
    "# Define batch size for querying transactions\n",
    "batch_size = 150\n",
    "\n",
    "def fetch_transactions(owner_batch):\n",
    "    \"\"\"\n",
    "    Fetches transactions for a batch of owners from BigQuery.\n",
    "\n",
    "    Parameters:\n",
    "    - owner_batch (list): List of owner card_no values for the batch.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: DataFrame containing transaction data for the owners in the batch.\n",
    "    \"\"\"\n",
    "    owner_str = ','.join(map(str, owner_batch))\n",
    "    transaction_query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM `{project_id}.{dataset_id}.transArchive_*`\n",
    "        WHERE card_no IN ({owner_str})\n",
    "    \"\"\"\n",
    "    return client.query(transaction_query).to_dataframe()\n",
    "\n",
    "# Save the transaction data in batches to avoid memory overload\n",
    "output_file = 'owner_transactions.csv'\n",
    "first_write = True\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    for i in range(0, len(owner_list), batch_size):\n",
    "        owner_batch = owner_list[i:i + batch_size]\n",
    "        transaction_df = fetch_transactions(owner_batch)\n",
    "        \n",
    "        # Write the transaction data to CSV\n",
    "        transaction_df.to_csv(f, header=first_write, index=False, mode='a', lineterminator='\\n')\n",
    "        first_write = False  # Ensure the header is only written once\n",
    "\n",
    "print(f\"Sampled transactions extracted and saved to {output_file}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
