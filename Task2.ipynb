{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley_26\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\cloud\\bigquery\\table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Riley_26\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\cloud\\bigquery\\table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Riley_26\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\google\\cloud\\bigquery\\table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled transactions extracted and saved to owner_transactions.csv\n",
      "Data size: 125.13076877593994 MB\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to your GBQ instance\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define the project and dataset\n",
    "project_id = \"umt-msba\"\n",
    "dataset_id = \"transactions\"\n",
    "\n",
    "# Control the sample size with a variable\n",
    "sample_size = 400 # Samping 400 owners for approximately 250MB sample\n",
    "\n",
    "# Sample owners directly within BigQuery\n",
    "owner_query = f\"\"\"\n",
    "    WITH unique_owners AS (\n",
    "        SELECT DISTINCT card_no\n",
    "        FROM `{project_id}.{dataset_id}.transArchive_*`\n",
    "        WHERE card_no != 3\n",
    "    )\n",
    "    SELECT card_no\n",
    "    FROM unique_owners\n",
    "    ORDER BY RAND()\n",
    "    LIMIT {sample_size}\n",
    "\"\"\"\n",
    "sampled_owners_df = client.query(owner_query).to_dataframe()\n",
    "\n",
    "# Convert owners to a list\n",
    "owner_list = sampled_owners_df['card_no'].tolist()\n",
    "\n",
    "# Define the batch size for the IN clause\n",
    "batch_size = 150\n",
    "\n",
    "# Function to query transactions for a batch of owners\n",
    "def fetch_transactions(owner_batch):\n",
    "    owner_str = ','.join(map(str, owner_batch))\n",
    "    transaction_query = f\"\"\"\n",
    "        SELECT * FROM `{project_id}.{dataset_id}.transArchive_*`\n",
    "        WHERE card_no IN ({owner_str})\n",
    "    \"\"\"\n",
    "    return client.query(transaction_query).to_dataframe()\n",
    "\n",
    "# Save results in batches to avoid memory overload\n",
    "output_file = 'owner_transactions.csv'\n",
    "first_write = True\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    for i in range(0, len(owner_list), batch_size):\n",
    "        owner_batch = owner_list[i:i+batch_size]\n",
    "        transaction_df = fetch_transactions(owner_batch)\n",
    "        \n",
    "        # Write to CSV\n",
    "        transaction_df.to_csv(f, header=first_write, index=False, mode='a', lineterminator='\\n')\n",
    "        first_write = False  # Ensure header is only written once\n",
    "\n",
    "print(f\"Sampled transactions extracted and saved to {output_file}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
